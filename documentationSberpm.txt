Process Mining с sberpm

Библиотека sberpm предназначена для анализа и исследования процессов, построения их в виде графов и решения смежных задач классификации и кластеризации с использованием алгоритмов машинного обучения.

Для получения полной версии библиотеки, напишите письмо с запросом на aabugaenko@sberbank.ru

Страница платформы SberPM: https://developers.sber.ru/portal/products/sber-process-mining

title
Оглавление

---------- Предварительная обработка данных ----------

I. Мэтчинг таблиц

II. Генератор логов # в полной версии библиотеки

III. Синтетические ID процесса # в полной версии библиотеки

IV. DataHolder---------- Автоматическое исследование с помощью искусственного интеллекта ---------- # в полной версии библиотеки

---------- Традиционный Process Mining ----------

I. Майнеры и визуализация графов

    SimpleMiner
    CausalMiner
    HeuMiner
    AlphaMiner
    AlphaPlusMiner
    InductiveMiner
    ML-miners
    NLP-Miner # в полной версии библиотеки
    Correlation-Miner
    II-miner

II. Метрики

    Метрики + графы

III. Conformance Checking

IV. BPMN

V. Визуализация

---------- Машинное-обучение ----------

I. Кластеризация этапов # в полной версии библиотеки

II. Автоматический поиск неэффективностей # в полной версии библиотеки

III. Поиск аномалий

IV. Факторный анализ # в полной версии библиотеки

V. Модуль автоматического построения моделей # в полной версии библиотеки

VI. Рекомендательная система # в полной версии библиотеки

VII. Анализ текстов # в полной версии библиотеки

VIII. Сентиментный анализ # в полной версии библиотеки

IX. Поиск счастливого пути # в полной версии библиотеки

X. Предсказание структуры графа # в полной версии библиотеки

XI. Имитационное моделирование и what-if анализ

XII. Decision Mining

XIII. Хронометраж
---------- Предварительная обработка данных ----------
I. Мэтчинг таблиц

Данный модуль sberpm.column_matching представляет собой систему для анализа значения столбцов в двух таблицах. Анализ производится по текстовым и численным колонкам. Колонки, у которых будут похожие значения в разных таблицах, будут сопоставляться.

Параметры, передаваемые в класс ColumnMatching:

    data_left (pandas DataFrame) - первая таблица для сравнения
    data_right (pandas DataFrame) - вторая таблица для сравнения
    p_value_threshold (float = default 0.05) - порог для p-value теста Колмогорова-Смирнова для численных колонок
    iou_threshold (float = default 0.5) - порог для метрики IOU для текстовых колонок

Методы класса ColumnMatching:

    get_result_pairs возвращает пары с похожими колонками из левой и правой таблице.
    get_result_table возвращает сводную таблицу, в которой по осям колонки из левой и правой таблице, а в ячейках меры похожести соответствующих колонок.

from pandas import DataFrame

from sberpm.column_matching import ColumnMatching

Тест на тривиальных синтетических данных

data_left = DataFrame({

    "names left": ["Alexander FFFFF", "Andrew Alexander", "Ilya Emilia", "Ilya Alexander"] * 50,

    "last names left": ["Arkhipov", "Kuznetsova", "Kuznetsova", "Bugaenko"] * 50,

})

data_right = DataFrame({

    "names right": ["Seva Alexander", "Alexander Vera", "Ilya Andrew"] * 50,

    "last names right": ["Zarubin", "Zarubin", "Kuznetsova"] * 50,

})


column_map = ColumnMatching(

    data_left,

    data_right,

    p_value_threshold=0.05,

    iou_threshold=0.25,

)

column_map.map_columns()

column_map.get_result_pairs()

column_map.get_result_table()

II. Генератор логов
В полной версии библиотеки

LogGenerator - позволяет автоматически генерировать логи процессов, так же его возможно настроить под конкретную задачу.
III. Синтетические ID процесса
В полной версии библиотеки

Pro_n_check нумерует экземпляры процессов в зависимости от заданных условий. Создает столбец 'pro_n'.
IV. DataHolder

DataHolder – это базовый класс для хранения данных. Практически все алгоритмы библиотеки работают с ним (принимают на вход).

Для создания класса DataHolder необходимо сперва указать путь к файлу или передать DataFrame конструктору, а затем указать id_column и activity_column. Однако, для большинства алгоритмов Process Mining, представленных в библиотеке, этих столбцов недостаточно – необходимы хотя бы одна колонка времени (start_timestamp_column и/или end_timestamp_column) и колонка пользователей (user_column).
Параметры DataHolder

    data (str or pd.DataFrame) – путь к файлу данных (.csv, .xls(x), .txt) или pd.DataFrame

    id_column (str) – столбец id

    activity_column (str) – столбец активностей

    *start_timestamp_column (str) – время начала активностей

    *end_timestamp_column (str) – время окончания активностей

    user_column (str) – столбец с именами/id пользователей

    text_column (str) – столбец с текстовыми данными

    duration_column (str) – столбец с длительностями активностей (если не задается, то расчитывается как время_активности_2 - время_активности_1, причем если есть только один столбец со временем, то для последней активности в цепочке ставится NaN)

    duration_unit (str) – размерность (единица измерения) значений в столбце duration_column, если он задан

    sep (str, default=',') – разделительный знак (используется только при чтении данных из файла)

    encoding (str) – кодировка (используется только при чтении данных из файла)

    nrows (int) – количество строк для чтения (используется только при чтении данных из файла)

    preprocess (bool, default=True) – предобработка данных (сортировка, удаление None-значений, преобразование типов)

    time_format (str) – формат временных колонок (обязательно задавать для правильного распознавания даты и ускорения работы). Правила написания: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes

    time_errors: (str, default='raise') – действие при ошибке конвертации

    dayfirst: (bool, default=None) – True, если день стоит в начале строки

    yearfirst: (bool, default=None) – True, если год стоит в начале строки

    n_jobs (int, default=1) – максимальное количество потоков, доступное для некоторых вычислений

* Для большинства алгоритмов нужно задать хотя бы один из временных столбцов. Если нет информации о типе столбца (время начала или конца), следует задать его как start_timestamp_column. Для верного распознавания формата также необходимо указать time_format.

Исследуемые данные должны представлять собой журнал событий (лог-файл), в котором хранится информация о последовательности (цепочке) событий (активностей) в бизнес-процессах. Пример журнала событий: 𝑊={(𝑎,𝑏,𝑐,𝑑),(𝑎,𝑐,𝑏,𝑑),(𝑎,𝑒,𝑑)}
, где события 𝑎, 𝑏, 𝑐, 𝑑 и 𝑒

сортируются по времени.
Создание DataHolder
– с помощью DataFrame

from sberpm import DataHolder

import pandas as pd


df = pd.DataFrame({'id_column': [1, 1, 2, 2, 3, 3],

                   'activity_column': ['st1', 'st2', 'st1', 'st3', 'st1','st2'],

                   'start_timestamp_column': ['10.05.2020', '10.09.2020', '10.03.2020', '10.04.2020', '10.05.2020', '10.05.2020']})


data_holder = DataHolder(data=df,

                         id_column='id_column',

                         activity_column='activity_column',

                         start_timestamp_column='start_timestamp_column',

                         time_format='%d.%m.%Y')

– с помощью указания пути файла

path = 'example.xlsx'

data_holder = DataHolder(data=path,

                         id_column='id',

                         activity_column='stages',

                         start_timestamp_column='dt',

                         user_column='users',

                         text_column="some_text",

                         time_format='%Y-%m-%d')

Если данные имеют какой-нибудь разделитель, например '|' как в csv, то после задания колонок, нужно задать параметр sep='|'.
Атрибуты DataHolder

В DataHolder названия столбцов хранятся в соответствующих переменных (т.е. нет необходимости запоминать названия колонок):

    id_column
    activity_column
    start_timestamp_column
    end_timestamp_column
    user_column
    text_column
    duration_column

Кроме того, в DataHolder хранятся исходные и сгруппированные данные в виде DataFrame, к которым можно обратиться следующим образом:

    data
    grouped_data

Методы DataHolder

    check_or_calc_duration – рассчитывает длительность каждой активности (в секундах), если это необходимо
    get_grouped_data – выводит сгруппированные данные по id и указанным колонкам (например, по activity_column и start_timestamp_column)
    get_unique_activities – выводит список уникальных активностей
    get_columns – выводит список с названиями колонок
    get_text – выводит колонку с текстом, если такая есть
    get_timestamp_col – выводит временную колонку; если их имеется 2, то выводит start_time_column
    is_interval – возвращает True, если это "интервальный лог" (у которого имеются обе временные колонки: начала и конца активности)
    top_traces_dh – возвращает data_holder с данными для n самых частых цепочек

data_holder.check_or_calc_duration()

data_holder.data.head()

data_holder.get_grouped_data(data_holder.activity_column, data_holder.start_timestamp_column).head()

dh_3 = data_holder.top_traces_dh(3)  # данные только для топ 3 цепочек

dfg = dh_3.get_grouped_data(dh_3.activity_column)

dfg.value_counts(dh_3.activity_column)  # проверка

Имея данные о бизнес-процессе с цепочками статусов и временем начала каждого из них, можно загрузить их в DataHolder и построить граф, максимально описывающий этот бизнес-процесс.
---------- Автоматическое исследование с помощью искусственного интеллекта ----------
В полной версии библиотеки
Данный модуль в автоматическом режиме создаёт исследование в виде презентации с отображением диаграмм и графиков, а так же делает выводы на основе полученных результатов.

Автоматическое исследование с помощью искусственного интеллекта (beta версия)
----------Традиционный Process Mining----------
I. Майнеры и визуализация графов

Для построения и отрисовки графа процесса в библиотеке реализовано несколько алгоритмов. Все они хранятся в модуле sberpm.miners и имеют один метод:

    apply – строит граф, который сохраняется в поле graph

1. SimpleMiner

SimpleMiner отрисовывает все ребра, найденные в логе (без какой-либо фильтрации).

В терминах Process Mining:

    Если хотя бы в одной цепочке активностей из лога за некоторой активностью 𝑋

непосредственно следует активность 𝑌 (цепочка вида ...𝑋𝑌...), то пишут 𝑋>𝑌 (𝑌 follows 𝑋

    , follows relation).

SimpleMiner рисует ребра между такими парами активностей 𝑋
и 𝑌, если выполняется 𝑋>𝑌

.

from sberpm.miners import SimpleMiner

# Создание объекта SimpleMiner. В конструктор подается DataHolder и параметры алгоритма

# (у данного майнера параметров нет)

simple_miner = SimpleMiner(data_holder)



# Запуск алгоритма построения графа

simple_miner.apply()



# Сохранение графа

graph = simple_miner.graph

Визуализация графа

Для визуализации графа следует использовать GraphvizPainter из модуля sberpm.visual

%matplotlib inline

from sberpm.visual import GraphvizPainter

Класс GraphvizPainter имеет методы:

    apply – принимает на вход граф, полученный с помощью майнера, и производит расчет для его отрисовки
    write_graph – сохраняет граф в требуемом формате (pdf, svg, gv, png)
    show – выводит граф в notebook

# Создание объекта GraphvizPainter

painter = GraphvizPainter()



# Расчет графа по результатам работы SimpleMiner

painter.apply(graph)



# Можно сохранить граф на жесткий диск в формате png, svg, pdf или gv

painter.write_graph('SimpleMiner.png', format='png')



# Можно вывыести граф в notebook

painter.show()

Класс Graph из модуля sberpm.visual имеет методы:

    get_nodes – получить все узлы
    get_edges – получить все ребра
    add_node_metric – добавить метрику, связанную с узлами графа
    add_edge_metric – добавить метрику, связанную с ребрами графа
    clear_node_metrics – удалить все метрики с нод (узлов)
    clear_edge_metrics – удалить все метрики с ребер

2. CausalMiner

CausalMiner основан на фильтрации ребер.

    Производные типы связей от 𝑋>𝑌

    :

    прямые связи (𝑋→𝑌

, causal relation) – это связи, где Х>𝑌 и не 𝑌>𝑋
параллельные связи(𝑋∥𝑌
, parallel relation) – это связи, где Х>𝑌 и 𝑌>𝑋
независимые связи (𝑋#𝑌
, independent) – это связи, где не 𝑋>𝑌 и не 𝑌>𝑋

CausalMiner рисует ребра между такими парами активностей 𝑋
и 𝑌, если выполняется 𝑋→𝑌

.

from sberpm.miners import CausalMiner

# Майнер

causal_miner = CausalMiner(data_holder)

causal_miner.apply()

graph = causal_miner.graph



# Отрисовка

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

3. HeuMiner

HeuMiner – это эвристический майнер, который удаляет наиболее редкие связи в зависимости от задаваемого порога (threshold).

Параметр threshold принимает значения от 0 до 1. Чем он больше, тем меньше ребер на графе (оставшиеся ребра считаются более важными).

Источник: https://www.researchgate.net/publication/229124308_Process_Mining_with_the_Heuristics_Miner-algorithm

from sberpm.miners import HeuMiner

# Майнер

heu_miner = HeuMiner(data_holder, threshold=0.8)

heu_miner.apply()

graph = heu_miner.graph



# Отрисовка

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

4. AlphaMiner

AlphaMiner рисует граф в виде сетей Петри с учетом прямых, параллельных и независимых связей между активностями.

from sberpm.miners import AlphaMiner

# Майнер

alpha_miner = AlphaMiner(data_holder)

alpha_miner.apply()

graph = alpha_miner.graph



# Отрисовка

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

5. AlphaPlusMiner

AlphaPlusMiner – имплементация Alpha+ майнера, который также рисует граф в виде сетей Петри с учетом связей, но в отличие от AlphaMiner может работать с одноцикловыми (one-loop) цепочками вида activity_1→

activity_1 (самоцикл).

from sberpm.miners import AlphaPlusMiner

# Майнер

alpha_miner_plus = AlphaPlusMiner(data_holder)

alpha_miner_plus.apply()

graph = alpha_miner_plus.graph



# Отрисовка

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

6. InductiveMiner

InductiveMiner создаёт дерево процесса. Лисья дерева - реальные активности процесса, остальные вершины - операторы. Есть 4 типа операторов:

    ПОСЛЕДОВАТЕЛЬНЫЙ (->),
    ИСКЛЮЧАЮЩЕЕ ИЛИ (X),
    ПАРАЛЛЕЛЬНЫЙ (||),
    ЦИКЛ (*).

Есть дополнительный 'оператор', который говорит о том, что было невозможно найти ни один из 4 операторов, представленных выше:

    СМЕШЕННАЯ МОДЕЛЬ ('?')

Замечание: некоторые из листьев дерева могут быть скрытыми активностями, отображаемыми чёрными прямоугольниками. Они не являются реальными активностями и используются только для сохранения правильной структуры дерева.

Например, из лога, состоящего из двух цепочек процесса 𝑊={(𝑎,𝑏,𝑐),(𝑎,𝑐)}

, можно получить следующее дерево процеса:
->(a, X(b, скрытая_активность), c).

Если во время очередной итерации алгоритм не может найти разрез графа (=подобрать один из 4 операторов), возможно добавить следующее поведение: если существует активность А, при удалении которой удаётся подобрать оператор, алгоритм возвращает следующее дерево:
||(X(активность_А, скрытая_активность), граф_без_активности_А) - то есть активность А считается параллельной остальному графу.

Это поведение может быть включено или выключено параметром parallel_activity в классе InductiveMiner.

from sberpm.miners import InductiveMiner

# Miner

inductive_miner = InductiveMiner(data_holder)

inductive_miner.apply()

graph = inductive_miner.graph



# Visualization

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

7. ML-miners

from sberpm.miners import MLMiner

ML-miner - майнер основанный на lasso-регрессии.

В данном майнере подсчитываются количество повторений каждого этапа в каждом процессе. Рассчитывается лассо-регрессия, в которой столбец одного вида деятельности рассматривается как цель, а остальные столбцы других видов деятельности - как признаки. На ребрах графа отображаются только те отношения, которые имеют абсолютное значение коэффициента лассо-регрессии больше, чем значение порогового значения.

ML-miner рисует ребра между такими парами активностей 𝑋
и 𝑌, если 𝑋 влияет на 𝑌

.

    data_holder : DataHolder Объект, содержащий журнал событий и имена его необходимых колонок.

    threshold: float (По умолчанию 0.05) Порог абсолютного значения коэффициента регрессии. Значения превышающие пороговое значение, считаются значимыми связями между деятельностью.

# Miner

ml_miner = MLMiner(data_holder, threshold=0.1)

ml_miner.apply()

graph = ml_miner.graph



# Visualization

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

Auto-miner

Auto-miner основан на Sequence Feature Selector.

Auto-miner рисует ребра между такими парами активностей 𝑋
и 𝑌, если 𝑋 влияет на 𝑌

.

    data_holder : DataHolder Объект, содержащий журнал событий и имена его необходимых столбцов.

from sberpm.miners import AutoMiner

# Miner

autominer = AutoMiner(data_holder)

autominer.apply()

graph = autominer.graph



# Visualization

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

8. NLP-miner
В полной версии библиотеки

NLP-miner использует для анализа этапов процесса библиотеку navec.

NLP-miner объединяет активности 𝑋
и 𝑌 и удаляет ребра между 𝑋 и 𝑌, если название 𝑋 похоже на 𝑌

.
9. Correlation-miner

Correlation-miner может создать граф журнала событий, в котором нет графы ID экземпляра процесса.

Граф будет иметь метрику 'count' для узлов и ребер.

Correlation-miner использует линейное программирование, чтобы восстановить утраченные экземпляры процесса на основе времени каждого этапа.

    data_holder: DataHolder Объект, содержащий журнал событий и имена его необходимых колонки.

    greedy: bool, default=True Если True, матрица длительности времени вычисляется жадным способом. Существует вероятность, что решение не будет оптимальным, но это значительно сокращает время и память для больших журналов.

    sparse: bool, default=False Используется только когда greedy=False. Если значение True, вычисление матрицы длительности будет производиться с использованием разреженной матрицы. Это может немного увеличить время вычислений, но немного уменьшить память.

from sberpm.miners import CorrelationMiner


# Miner

corr_miner = CorrelationMiner(data_holder)

corr_miner.apply()

graph = corr_miner.graph

# Visualization

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

10. II-miner

Майнер для анализа параллельных действий. Подсчитывает пересечения по временным меткам действий. Параллельные этапы - это этапы с такими экземплярами процессов, что время их начала и окончания перекрывается. Добавлен узел parallel_metric, который показывает количество пересечений времени на этом этапе и других этапах. От синих узлов стрелки указывают на параллельные этапы.

Параметры: data_holder: DataHolder Объект, содержащий журнал событий и имена его необходимых столбцов.

Аргументы: graph: добытый график процесса для рисования

from sberpm.miners import ParallelMiner

# Miner

parallel_miner = ParallelMiner(data_holder)

parallel_miner.apply()

graph = parallel_miner.graph

# Visualization

painter = GraphvizPainter()

painter.apply(graph)

painter.show()

II. Метрики

На данный момент в модуле sberpm.metrics есть 5 основных типов метрик:

    ActivityMetric – метрики по активностям (группировка по activity_column)
    TransitionMetric – метрики по переходам (группировка по уникальным переходам)
    IdMetric– метрики по id (группировка по id_column)
    TraceMetric – метрики по цепочкам активностей (группировка по уникальным цепочкам)
    UserMetric – метрики по пользователям (группировка по user_column)

from sberpm.metrics import ActivityMetric, TransitionMetric, IdMetric, TraceMetric, UserMetric

Параметры:

    data_holder – объект типа DataHolder, для которого надо рассчитать метрики
    time_unit – единица времени, по умолчанию расчет временных метрик происходит в часах
    round – количество цифр после запятой (только для метрик, значения которых могут быть с плавающей точкой)

Общие методы для всех классов:

    apply – расчет всех характеристик

    calc_metrics(...) – расчет указанных метрик (соответствуют методам/названиям колонок в DataFrame из apply)

    calculate_time_metrics – расчет временных характеристик

    total_duration – расчет суммарного времени работы

    min_duration – расчет минимального времени работы

    max_duration – расчет максимального времени работы

    mean_duration – расчет среднего времени работы

    median_duration – расчет медианного времени работы

    std_duration – расчет стандартного отклонения времени работы

    var_duration – расчет дисперсии времени работы

Дополнительные методы:

    ActivityMetric
        count - сколько раз активность встречается в логе
        unique_ids - уникальные id для каждой активности
        unique_ids_num - количество уникальных id для каждой активности
        aver_count_in_trace - среднее количество раз встречаемости активности в цепочке
        loop_percent - процент зацикленности
        throughput - частота - количество выполненных активностей за единицу времени
        unique_users - уникальные пользователи, работавшие с данной активностью
        unique_users_num - количество уникальных пользователей, работающих над данной активностью
        success_rate(...) - доля id, имеющих данную активность, которая выполнилась успешно (закончились успешными активностями)
        failure_rate(...) - доля id, имеющих данную активность, которая выполнилась неуспешно (закончились неуспешными активностями)

    IdMetric
        trace - цепочка (список активностей)
        trace_length - длина цепочки (кол-во активностей в цепочке)
        unique_activities - уникальные активности в цепочке
        unique_activities_num - количество уникальных активностей в цепочке
        loop_percent - процент зацикленности
        unique_users - уникальные пользователи, работающие с этим ID
        unique_users_num - кол-во уникальных пользователей, работавших с данным ID

    TraceMetric
        count - сколько раз данная цепочка встречается в логе
        ids - уникальные id с данной цепочкой
        trace_length - длина цепочки (кол-во активностей в цепочке)
        unique_activities - уникальные активности в цепочке
        unique_activities_num - количество уникальных активностей в цепочке активностей
        unique_users - уникальные пользователи, работающие над цепочкой активностей
        unique_users_num - количество уникальных пользователей, работающих над цепочкой активностей

    TransitionMetric
        count - сколько раз данный переход встречается в логе
        unique_ids - уникальные id для каждого перехода
        unique_ids_num - количество уникальных id для каждого перехода
        aver_count_in_trace - среднее количество раз встречаемости объекта в цепочке
        loop_percent - процент зацикленности
        throughput - частота - количество выполненных переходов за единицу времени
        unique_users - уникальные пользователи, работающие над объектом
        unique_users_num - кол-во уникальных пользователей, работающих над объектом
        success_rate(...) - доля id, имеющих текущий переход, которые выполнились успешно (закончились успешными активностями)
        failure_rate(...) - доля id, имеющих текущий переход, которые выполнились неуспешно (закончились неуспешными активностями)

    UserMetric
        count - сколько раз данный пользователь встречается в логе
        unique_activities - уникальные активности, с которыми работал пользователь
        unique_activities_num - количество уникальных активностей, с которыми работал пользователь
        unique_ids - уникальные id с данным пользователем
        unique_ids_num - количество уникальных id с данным пользователем
        throughput - число раз выполнения объекта за единицу времени
        workload - доля активности лога, выполненных данным пользователем

1. ActivityMetric

# Создание объекта ActivityMetric

activity_metric = ActivityMetric(data_holder, time_unit='d')

# Расчет всех метрик

activity_metric.apply().head()

2. TransitionMetric

# Создание объекта TransitionMetric

transition_metric = TransitionMetric(data_holder, time_unit='d')

# Расчет всех метрик

transition_metric.apply().head()

3. IdMetric

# Создание объекта IdMetric

id_metric = IdMetric(data_holder, time_unit='d')

# Расчет всех метрик

id_metric.apply().head()

4. TraceMetric

# Создание объекта TraceMetric

trace_metric = TraceMetric(data_holder, time_unit='d')

# Расчет всех метрик

trace_metric.apply().head()

5. UserMetric

# Создание объекта UserMetric

user_metric = UserMetric(data_holder, time_unit='d')

# Расчет всех метрик

user_metric.apply().head()

Метрики + графы

В библиотеке реализована возможность представить ряд метрик на графе. Сделать это можно в классе Graph с помощью методов:

    add_node_metric – добавить метрику, связанную с узлами графа
    add_edge_metric – добавить метрику, связанную с ребрами графа

# Расчет метрик

nodes_count_metric = activity_metric.count().to_dict()

edges_count_metric = transition_metric.count().to_dict()

mean_time_node_metric = activity_metric.mean_duration().fillna(0).to_dict()

# Получение графа из майнера

graph = causal_miner.graph

# Добавление метрик на граф

graph.add_node_metric('count', nodes_count_metric)

graph.add_edge_metric('count', edges_count_metric)

graph.add_node_metric('mean_time', mean_time_node_metric)

# Создание объекта GraphvizPainter

painter = GraphvizPainter()

# Отрисовать граф и связать цвет узлов и ребер с нужными метриками

painter.apply(graph, node_style_metric='count', edge_style_metric='count')

# или painter.apply(graph, node_style_metric='mean_time', edge_style_metric='count')

# Сохранение графа

painter.write_graph("metric_graph.png", format = 'png')

# Отображение в jupyter-notebook

painter.show()

Чтобы удалить метрики с графа, следует воспользоваться следующими методами:

    clear_node_metrics – удалить все метрики с нод (узлов)
    clear_edge_metrics – удалить все метрики с ребер

graph.clear_node_metrics()

graph.clear_edge_metrics()

III. Conformance Checking
TokenReplay

TokenReplay позволяет рассчитать fitness, который показывает, насколько хорошо граф описывает бизнесс-процесс (1 – хорошо, 0 – плохо). Fitness вычисляется отдельно для каждой цепочки (id) при ее проигрывании по сети Петри по следующей формуле:
𝐹𝑖𝑡𝑛𝑒𝑠𝑠=12(1−𝑚𝑖𝑠𝑠𝑒𝑑𝑐𝑜𝑛𝑠𝑢𝑚𝑒𝑑)+12(1−𝑟𝑒𝑚𝑎𝑖𝑛𝑖𝑛𝑔𝑝𝑟𝑜𝑑𝑢𝑐𝑒𝑑)

    produced tokens – появились в результате перехода
    consumed tokens – удалились в результате перехода
    remaining tokens – остались в конце проигрывания
    missing tokens – не было, но они необходимы для проигрывания, поэтому их вставляют

Библиотека выдает следующие метрики:

    значения 4 величин и fitness каждой цепочки
    усредненный fitness по всем цепочкам (mean_fitness)
    fitness по всему логу – в формулу подставляются суммарные значения 4 величин по всем цепочкам в логе (average_fitness)

from sberpm.conformance_checking import TokenReplay

token_replay = TokenReplay(data_holder, alpha_miner.graph)

token_replay.apply()

token_replay.result

print('mean:', token_replay.mean_fitness)

print('average:', token_replay.average_fitness)

Также в библиотеке доступен более общий класс ConformanceChecking, сдержащий TokenReplay и рад других метрик:

    precision
    generalization
    simplicity

from sberpm.conformance_checking import ConformanceChecking

cc = ConformanceChecking(data_holder, alpha_miner.graph)

cc.get_conformance_checking()

cc.get_fitness_df()

IV. BPMN

Для сохранения графа в формате BPMN (Business Process Model and Notation) можно воспользоваться BpmnExporter из модуля sberpm.bpmn. Он имеет следующие методы:

    apply_petri – построить BPMN для сети Петри
    get_string_representation – получить BPMN-нотацию графа
    write – записать граф в BPMN формате

На данный момент сохранять можно только графы, полученные из Alpha Miner.

from sberpm.bpmn import BpmnExporter

bpmn_exporter = BpmnExporter()

bpmn_exporter.apply(alpha_miner.graph)

bpmn_exporter.get_string_representation()[:1000]

bpmn_exporter.write('exported.bpmn')

Для загрузки BPMN-файла есть класс BpmnImporter со следующими методами:

    load_bpmn_from_xml – загрузить граф, представленный в виде BPMN
    get_pydotplus_graph – получить граф в формате pydotplus

from sberpm.bpmn import BpmnImporter

bpmn_importer = BpmnImporter()

bpmn_importer.load_bpmn_from_xml('exported.bpmn')

pydot_graph = bpmn_importer.get_pydotplus_graph()

pydot_graph.write('imported_bpmn.svg', prog='dot', format='svg')


V. Визуализация

Класс ChartPainter из модуля sberpm.visual предназначен для создания основных типов графиков. В основе визуализации лежит библиотека plotly, благодаря чему все диаграммы являются интерактивными.

from sberpm.visual import ChartPainter

Параметры:

    data – данные, которые необходимо визуализировать (DataFrame, DataHolder или объекта класса метрик)
    template – стиль графиков, по умолчанию plotly
    palette – цветовая палитра графиков, по умолчанию sequential.Sunset_r

Каждый метод ChartPainter позволяет отрисовать график определенного типа:

    hist – гистограмма
    bar – столбчатая диаграмма
    box – ящичковая диаграмма
    scatter – диаграмма рассеяния
    line – линейный график
    pie – круговая диаграмма
    sunburst – диаграмма солнечные лучи
    heatmap – 2D гистограмма
    timeline – диаграмма Ганта
    pareto – диаграмма Парето

Основные параметры методов (для более подробной информации см. документацию):

    x, y – названия столбцов для отрисовки по осям X и Y соответственно
    sort – название столбца для сортировки значений
    n – количество строк для визуализации
    color – название столбца для задания цвета элементам графика
    subplots – кортеж вида (rows, cols, ncols), где rows и cols – это названия столбцов для отрисовки нескольких графиков по рядам и столбцам соответственно, а ncols – это количество столбцов
    text – название столбца с текстовой информацией (или ее вид) для отображения на графике
    orientation – ориентация графика
    opacity – прозрачность элементов графика
    edge – границы элементов графика
    title – название графика

Каждый метод прост в использовании, но при этом обладает достаточно широким функционалом, который позволяет построить графики для любых задач.
Гистограмма

painter = ChartPainter(id_metric)

painter.hist(x='total_duration', edge=True)

Столбчатая диаграмма

painter = ChartPainter(user_metric)

painter.bar(x=data_holder.user_column, y='total_duration', text=True)

Диаграмма рассеяния

painter = ChartPainter(id_metric)

painter.scatter(x='mean_duration', y='median_duration', color='unique_users_num', size='trace_length',

                edge=True, opacity=0.8)

### Круговая диаграмма

painter = ChartPainter(user_metric)

painter.pie(labels='count', n=15)

Гистограмма распределения активностей по диапазонам времени
По всем активностям

painter = ChartPainter(data_holder)

painter.hist_activity_of_dur(top= False, use_median=False)

По топ активностям

painter.hist_activity_of_dur(top= True)

По одной активности

painter.hist_activity_of_dur(by_activity='Stage_6')

----------Машинное обучение----------
I. Кластеризация этапов
В полной версии библиотеки

Данный модуль служит для кластеризации этапов процесса, для нахождения близких или идентичных этапов процессов.
II. Автоматический поиск неэффективностей
В полной версии библиотеки

Модуль автоматического поиска неэффективностей sberpm.autoinsights позволяет в автоматическом режиме выявить слабые места и уязвимости процесса и наглядно продемонстрировать их на графе процесса. При анализе классом AutoInsights учитываются такие факторы, как:

    Длительность этапа
    Рост длительности этапа
    Нерегулярность (редкость) этапа
    Этап имеет bottleneck c низкой вариативностью
    Этап имеет bottleneck c высокой вариативностью
    Этап имеет большую длительность из-за частых повторений инцидентов
    Этап имеет большую длительность из-за разовых инцидентов
    Этап приводит к росту времени процесса и/или прочих этапов
    Этап проходит с ошибками, что приводит к замедлению процесса
    Этап проходит с критическими ошибками системы, что приводит к неуспеху процесса
    Этап проходит со структурными ошибками, что приводят к неуспеху процесса
    Сторнирование на данном этапе приводит к неуспеху замедлению процесса
    Сторнирование на данном этапе приводит к неуспеху процесса
    Уровень аномальности
    Сумма финансовых эффектов

III. Поиск аномалий

Выявление аномалий (также обнаружение выбросов) — это распознавание во время интеллектуального анализа данных редких данных, событий или наблюдений, которые вызывают подозрения ввиду существенного отличия от большей части данных.

Для поиска аномалий (выбросов) в данных в библиотеке есть модуль sberpm.ml.anomaly_detection, в котором есть классы OutlierCBLOF, OutlierForest, OutlierLOF, OutlierOCSVM, OutlierCustom, OutlierEnsemble. В каждом классе реализован свой алгоритм выявления аномалий без учителя, который обнаруживает аномалии в непомеченных наборах данных при предположении, что большая часть набора данных нормальна, путем поиска представителей, которые меньше подходят к остальному набору данных.

OutlierEnsemble это композиция алгоритмов обнаружения аномалий, итоговой ответ которой представляет собой голосование (объект считается выбросом, если большинство алгоритмов определило его как выброс) следующих алгоритмов: KNN, ABOD, HBOS, Isolation Forest.

from sberpm.ml.anomaly_detection import OutlierCBLOF, OutlierForest, OutlierLOF, \

                                        OutlierOCSVM, OutlierCustom, OutlierEnsemble

В качестве параметра объект принимает на вход DataHolder, по которому рассчитывает базовые статистики, такие как среднее время, длина цепочки активностей, число уникальных пользователей (если они есть) и т. д.

outlier_detector = OutlierForest(data_holder)

Каждый класс имеет следующие методы:

    add_feature – добавление признака, по которому требуется найти аномалии
    add_groupby_feature – добавление признака для поиска аномалий, рассчитанного по сгруппированным данным
    apply – запуск алгоритма
    get_outlier_ids – вывод id аномальных процессов
    print_result – вывод статистики по аномалиям
    show_permutation_importance – иллюстрация permutation importance признаков, по которым отличаются выбросы (работает везде кроме OutlierEnsemble)

# Добавление признака с именем max_time, вычисляемого путем применения функции max к колонке

# data_holder.duration_column в сгруппированных по id данных (масимальное время активности в процессе)

outlier_detector.add_groupby_feature('max_time', data_holder.duration_column, max)

В модуле реализовано 5 техник выявления аномалий:

    Isolation Forest (IF)
    One-Class Support Vector Machines (OCSVM)
    Local Outlier Factor (LOF)
    Cluster-Based Local Outlier Factor (CBLOF)
    OutlierEnsemble в котором есть KNN, HBOS, ABOD, Isolation Forest

Также можно использовать любой другой алгоритм поиска аномалий (например, из библиотеки pyod).
Isolation Forest

    В основе техники изолирующего леса лежит идея о том, что аномальные наблюдения легче отделить от остальных (нормальных) объектов датасета. Алгоритм строит ансамбль изолирующих бинарных деревьев решений, в каждом узле которого выбор признака и порога разбиения производится случайным образом. Дерево строится до тех пор, пока в листе не останется только один объект или объекты с одинаковыми значениями. Интуитивно понятно, что аномальные точки – это те, что имеют меньшую длину пути в дереве, которая определяется как число ребер, которые объект проходит от корневого узла до листа.

outlier_detector = OutlierForest(data_holder)

One-Class Support Vector Machines

    Основная идея классического метода опорных векторов (SVM) заключается в разделении объектов, относящихся к разным классам, гиперплоскостью так, чтобы максимизировать расстояние между ними. Алгоритм OCSVM, как следует из названия, обучается на данных, принадлежащих одному классу – классу нормальных объектов. Он определяет границы этих объектов и классифицирует все остальные точки, лежащие по другую сторону от разделяющей поверхности, как аномальные.

outlier_detector = OutlierOCSVM(data_holder)

Local Outlier Factor

    Локальный уровень выброса (LOF) основывается на концепции локальной плотности объекта, где локальность задается его 𝑘

    ближайшими соседями, расстояния до которых используются в качестве оценки плотности. Путем сравнения локальной плотности объекта с локальной плотностью его соседей, можно выделить области с аналогичной плотностью и точки, которые имеют существенно меньшую плотность, чем ее соседи. Эти точки считаются выбросами.

outlier_detector = OutlierLOF(data_holder)

Cluster-Based Local Outlier Factor

    В отличие от стандартного LOF, основанном на метрическом подходе к выявлению локальных выбросов, CBLOF выявляет кластерную структуру данных, разделяет кластеры на "большие" и "малые" и затем определяет локальность малых кластеров по отношению к большим. Кластеры, чья локальность по отношению к другим мала, определяются как выбросы.

outlier_detector = OutlierCBLOF(data_holder)

Помимо указанных 4 алгоритмов поиска аномалий, можно воспользоваться любым другим, который не встроен в библиотеку, но есть в pyod – например, histogram-based outlier detection (HBOS).

from pyod.models.hbos import HBOS

hbos = HBOS(contamination=0.1)

outlier_detector = OutlierCustom(data_holder, hbos, outlier_label=1)

После выбора алгоритма, его следует применить с помощью метода apply.

outlier_detector.apply()

Результаты представляют собой список id аномалий (метод get_outlier_ids), таблицу с описательными статистиками по аномальным и нормальным объектам (метод print_result), а также графическую иллюстрацию важности использованных для поиска аномалий признаков (метод show_permutation_importance).

outlier_detector.get_outlier_ids()

outlier_detector.print_result()

outlier_detector.show_permutation_importance()

OutlierEnsemble

    OutlierEnsemble это композиция алгоритмов обнаружения аномалий, итоговой ответ которой представляет собой голосование (объект считается выбросом, если большинство алгоритмов определило его как выброс) следующих алгоритмов: KNN, ABOD, HBOS, Isolation Forest. В качестве алгоритмов можно выбрать любое подмножетво из {"HBOS", "ABOD", "KNN", "IForest"}.

outlier_detector = OutlierEnsemble(data_holder, ["HBOS", "ABOD", "KNN", "IForest"])

outlier_detector.apply()

outlier_detector.print_result()

IV. Факторный анализ
В полной версии библиотеки

Факторный анализ – это многомерный метод, применяемый для изучения взаимосвязей между значениями переменных. Предполагается, что известные переменные зависят от меньшего количества неизвестных переменных и случайной ошибки. С помощью факторного анализа возможно выявление скрытых переменных факторов, отвечающих за наличие линейных статистических корреляций между наблюдаемыми переменными.
V. Модуль автоматического построения моделей
В полной версии библиотеки

Модуль автоматического построения моделей позволяет использовать методы машинного обучения для предсказания временных рядов и панельных данных.
VI. Рекомендательная система
В полной версии библиотеки

Данный модуль представляет собой рекомендательную систему, ранжирующую этапы процесса в порядке их приоритетности для реинжениринга. Система показывает, на какие активности нужно обратить внимание в первую очередь при оптимизации процесса. Сначала выбирается лучшая модель, описывающую зависимость таргетной метрики (metric) каждой из Активностей от признаковых метрик (metric_f) всех остальных Активносетй. В качестве метрик могут выступать количество рециклов (количество появлений активности, "appearance"), время выполнения("time"), количество повторений ("recycles"), кастомная метрика пользователя ("user_metric"). Далее на основании метрики строятся коэффициенты проблемности, в порядке убывания которых ранжируются Активности.
VII. Анализ текстов
В полной версии библиотеки

Данный модуль служит для кластеризации текстов. Для каждого кластера алгоритм выдает номер кластера (название кластера или самое близкое сообщение к центроиду кластера), и 10 самых распространенных слов в кластере.
VIII. Сентиментный анализ
В полной версии библиотеки

Данный модуль представляет собой систему для анализа тональности словесных комментариев в текстовом поле. Модуль анализа тональности имеет два режима: «базовый» и «продвинутый». В «базовом» режиме тональность текста определяется как «positive» или «negative», численное значение тональности определяется в пределах от -1 до 1 (от негатива к позитиву).
IX. Поиск счастливого пути
В полной версии библиотеки

Задачу поиска счастливого пути можно решить с помощью обучения с подкреплением (RL), которое по своей сути адаптировано к поиску оптимальных действий. RL работает с двумя объектами: агентом и средой. Во время обучения агент взаимодействует с окружающей средой, совершает действия и получает обратную связь, которая называется вознаграждением. Задача формулируется в рамках марковского процесса принятия решений, который основан на:

    множестве состояний в мире
    множестве действий
    вероятностном распределении следующего состояния при условии текущего состояния и завершенного действия
    награды при переходе между состояниями при выполнении действий.

Выполнение марковского свойства состоит в том, что следующее состояние условно не зависит от прошлых состояний и действий, учитывая текущее состояние и действие. Граф процесса рассматривается как среда, состояния — узлы графа (активности), действия — ребра (выбор следующей активности для перехода), награда — среднее отрицательное время перехода между прошлым и настоящим состояниями. При наличии ключевых состояний за переход в них также начисляется награда. Цель состоит в том, чтобы выбрать оптимальную политику — отображение из пространства состояний в пространство действий или, другими словами, руководство к действию в каждом состоянии — которое максимизирует ожидаемую дисконтированную сумму вознаграждений, проходящих через граф процесса. Оптимальная политика и, как следствие, путь находятся при помощи AutoRL с использованием лучшего по дисконтированной сумме наград метода из: value iteration, Q-learning, cross entropy, genetic algorithm.
X. Предсказание структуры графа
В полной версии библиотеки

Модуль GSPredictor (graph structure predictor) содержит алгоритм предсказания структуры графа, а именно, предсказываются две величины: вероятности и средние времена выполнения нод и рёбер графа. Эти величины представляются как временные ряды, получаемые из имеющихся данных, далее используются ml- и специализированные для работы с временными рядами алгоритмы для предсказания.
XI. Имитационное моделирование и what-if анализ

import os

import pandas as pd

from sberpm import DataHolder

from sberpm.visual import GraphvizPainter

from sberpm.miners import SimpleMiner, CausalMiner

from sberpm.metrics import ActivityMetric, TransitionMetric



%matplotlib inline



data = pd.read_excel('example.xlsx')

data.head()


dh = DataHolder(

    data=data,

    id_column='id',

    activity_column='stages',

    start_timestamp_column='dt'

)

Имитационное моделирование и what-if анализ

Модуль имитационного моделирования sberpm.imitation позволяет симулировать процесс в as-is форме, вносить изменения в процесс и проводить what-if моделирование, а также оценивать схожесть симуляции по сравнению с исходным лог-файлом с помощью классов Simulation и SimilarityMetric соответственно.

Класс Simulation содержит алгоритм симуляции идентичного лога процесса, а так же инструменты позволяющие изменять структуру графа процесса.

Шаги алгоритма:

I. Подготовка данных

    Генерация cross table с вероятностями переходов в ячейках таблицы
        В данные добавляются 'start' и 'end' события, они не отображаются в логах, но помогают при изменении структуры графа процесса (например если требуется добавить альтернативный вход в процесс, то используется следующая функция : add_edge('start', 'node_name', prob=float))
        Создается матрица смежности (с количеством переходов в ячейках), после значения приводятся к вероятностям

    Экстрагирование временных данных из оригинального лога, для сохранения картины распределения времени активностей.
        Из переданного data_holder сохраняются значения duration по активностям (далее они будут использованы при генерации новых этапов с идентичным распределением duration)

II. Генерация

    Данные таблицы преобразовываются в словарь {'action' : np.array({'action' : 'prob'})}, считается интервал между временными точками входа в процесс (нужен чтобы получить start_timestamp для каждой итерации)

    В цикле генерируются логи процесса
        На каждой итерации рекурсивно строится цепочка активностей
        В лог записываются активности, время начала процесса и id процесса

    Генерация времени (длительности этапов):
        Если активность есть в оригинальном data_holder, то эти данные сжимаются\растягиваются до количества этого этапа в сгенерированном логе
        Если активность отсутствует и при создании установлен like_node, то время генерируется с распределением, как у указанной активности (время будет масштабировано в зависимости от указанного mean_time)
        Если активность отсутствует и при создании не указан like_node, то берется значение среднего времени (или значение среднего по всем средним времени процесса, если при добавлении активности не было указано) и строится нормальное распределение.

III. Изменение структуры графа

Ниже указанны основные функции для работы со структурой графа.

    Удаление вершин и активностей
        delete_node(node, save_con=True) - удаляет активность по имени, флаг save_con означает будут ли сохранены соединения. Пример: A -> B -> C delete_node('B', save_con=True) A -> C
        delete_edge(node, node, side='right') - удаляет ребра (переходы) между переданными активностями, т.к связь может быть в обе стороны, применяется флаг side означающий будет ли связь слева направо, справа налево или в обе стороны. Пример: A <-> B delete_edge('A', 'B', side='right') A <- B

    Добавление ребер и активностей
        add_node(new_node, nodes=list, probabilities=list, mean_time=float, time_like='node', side='both') - добавление и связывание новой активности с переданным списком активностей, probabilities - вероятности перехода, mean_time - среднее время выполнения активности, side - в какую сторону будет связь, time_like - копирует распределение уже существующей активности (будет масштабировано в зависимости от mean_time)
        add_edge(node, node, prob, side) - добавление связи между активностями, prob - вероятности перехода, side - в какую сторону будет связь
        swap_nodes(node, node, save_probabilities=True) - меняет местами активности (вместе с распределением времени), save_probabilities - флаг указывающий на то, будут ли тянуться вероятности предшествующих нод при свапе активностей (если False, то производится перестановка и пересчет вероятностей)

    Изменение длительности и вероятностей
        change_edge_prob(node, node, prob=float) - изменяет вероятность перехода между активностями ( prob = 0, аналогичен удалению перехода)
        scale_time_node(node, scale=1) - изменяет время процесса (распределение останется прежним - как в оригинальном датасете), scale - это коэффициент уменьшения (если < 1) или увеличения (если > 1) продолжительности выполнения активности.

from sberpm.imitation import Simulation, SimilarityMetric

Параметры Simulation

    data_holder : DataHolder

Методы класса Simulation:

    generate – запуск симуляции iterations (количество) цепочек активностей (id)
    _mean_duration – средняя длительность выполнения активностей или переходов в процессе
    scale_time_node – изменение времени выполнения активности
    change_edge_probability - изменяет вероятность перехода (ребра) в процессе
    delete_node, delete_edge, delete_loop, delete_all_loops – удаление ноды и ребра из процесса (delete_loop - удаляет ребро "в себя")
    add_edge, add_node, add_loop - добавление ноды и ребра в процесс (loop - добавляет ребро "в себя")
    swap_nodes – меняет местами ноды (с сохранением или с пересчетом вероятностей)
    get_result – возвращает результаты генерации
    compute_metric - считает различные метрики
    get_probabilities_tab - возвращает матрицу смежности, позволяет увидеть вероятности переходов

На вход класс принимает объект типа DataHolder.

# Инициализация

sim = Simulation(dh)

As-is моделирование

Метод generate запускает симуляцию процесса. Он имеет параметры iterations – число цепочек событий для симуляции.

sim.generate(1000)

sim_data = sim.get_result()

sim_data.head()

generated_dh = DataHolder(

    data=sim_data,

    id_column='id',

    activity_column='stages',

    start_timestamp_column='dt',

)

Полученный лог можно отрисовать с помощью майнера и встроенного инструмента для визуализации графов.

simple_miner = SimpleMiner(generated_dh)

simple_miner.apply()

graph = simple_miner.graph



painter = GraphvizPainter()

painter.apply(graph)

painter.show()

What-if анализ

Методы delete_node, delete_edge, add_node, add_edge, add_loop, delete_loop и swap_nodes позволяют изменять граф процесс. После запуска симуляции процесс будет реализовываться по альтернативным путям.

# удаление нод

sim.delete_node("Stage_5")

# удаление ребер (side=True - только в одну сторону)

sim.delete_edge('Stage_7', 'Stage_5', side='right')

# добавление ноды (action), nodes - лист связанных вершин с новой вершиной, probabilities - вероятности перехода в связанные вершины

sim.add_node('Stages_10',

             nodes=['Stage_8', 'Stage_5', 'Stage_10'],

             probabilities=[0.15, 0.35], mean_time=3333, side='both')

# добавление вершины, prob - вероятность перехода в нее

sim.add_edge('Stage_7', 'Stage_7', prob=0.15, side='right')



# работа с петлями (можно писать и sim.add_edge('node1', 'node1', prob=0.5))

sim.delete_loop('Stage_10')



# меняет местами ноды и их среднее время (вероятности не меняет)

sim.swap_nodes('Stage_2', 'Stage_0', save_probabilities=False)



sim.generate(5000)



sim.get_result()

Для того чтобы контролировать то, как изменяются вероятности переходов между action в графе процесса, был добавлен метод get_probabilities_tab. Он возвращает таблицу переходов : столбец слева - это action "откуда", строка сверху - это "куда" делается переход, значения в ячейках это вероятности.

sim.get_probabilities_tab()



sim.change_edge_probability('Stage_10', 'Stage_10', 3)

sim.change_edge_probability('Stage_9', 'end', 0.75)

С помощью метода scale_time_node можно изменить время процесса (обратите внимание, распределение останется прежним - как в оригинальном датасете), scale - это коэффициент уменьшения (если < 1) или увеличения (если > 1) продолжительности выполнения активности

sim.scale_time_node('Stage_4', scale=1.5)

sim.scale_time_node('Stage_3', scale=0.75)



sim.generate(10000)

sim_data = sim.get_result()

sim_data.shape



generated_dh = DataHolder(

    data=sim.get_result(),

    id_column='id',

    activity_column='stages',

    start_timestamp_column='dt'

)

simple_miner = SimpleMiner(generated_dh)

simple_miner.apply()

graph = simple_miner.graph



painter = GraphvizPainter()

painter.apply(graph)

painter.show()

XII. Decision Mining

Модуль sberpm.decision_mining предназначен для проведения decision point analysis, который заключается в определении причин, почему процесс идет по тому или иному пути. Класс DecisionMining выявляет, как те или иные свойства (атрибуты) процесса влияют на выбор конкретного пути.

from sberpm.decision_mining import DecisionMining

На вход DecisionMining принимает объект типа DataHolder.

# Инициализация

dm = DecisionMining(data_holder)

DecisionMining имеет следующие методы:

    print_decision_points – выводит decision points (активности, после которых идет выбор)
    apply – выполняет анализ decision points, строя дерево решений по указанным атрибутам
    get_clf_metrics – выводит метрики классификации
    plot_confusion_matrix – рисует матрицы ошибок
    plot_feature_importance – рисует важность признаков в дереве
    plot_feature_distribution – рисует распределение признаков по классам
    plot_decision_tree – рисует дерево решений
    print_decision_rule – выводит решающие правила

Decision points – точки, где процесс имеет разветвление, можно посмотреть с помощью метода print_decision_points.

dm.print_decision_points()

Метод apply запускает алгоритм decision mining. Он имеет следующие параметры:

    categorical_attrs – названия категориальных признаков
    noncategorical_attrs – названия некатегориальных признаков
    decision_points – точки, по которым необходимо построить деревья решений, по умолчанию рассматриваются все
    sampling – нужен ли sampling (over- или under-), следует использовать в случае несбалансированных классов
    tree_params – параметры дерева решений
    grid_search – нужен ли подбор оптимальных гиперпараметров дерева решений
    param_grid – сетка параметров, используется только при grid_search=True
    random_state – используется в дереве решений и sampling
    n_jobs – используется в sampling и grid_search

dm.apply(categorical_attrs=[data_holder.user_column],

         noncategorical_attrs=[data_holder.duration_column],

         decision_points='all',

         sampling='RandomOverSampler',

         tree_params='default',

         grid_search=False,

         param_grid='default',

         random_state=42,

         n_jobs=None)

Результаты классификации можно посмотреть с помощью методов get_clf_metrics, plot_confusion_matrix и plot_feature_importance.

dm.get_clf_metrics()

Качество невысокое из-за особенностей синтетического датасета.

Все plot методы имеют параметры:

    decision_points – точки, для которых необходимо изобразить харатеристики
    savefig – нужно ли сохранить изображение

dm.plot_confusion_matrix(decision_points=['Stage_0'], savefig=False)

dm.plot_feature_importance(decision_points=['Stage_0'], savefig=False)

Метод plot_feature_importance дополнительно имеет еще два параметра:

    drop_outliers – нужно ли удалить выбросы для количественных признаков
    clf_results – нарисовать распределение признаков по результатам классификации (True) или по исходному логу (False)

dm.plot_feature_distribution(decision_points=['Stage_0'], drop_outliers=True, clf_results=True, savefig=False)

Методы plot_decision_tree и print_decision_rule выводят результаты работы decision mining алгоритма в виде дерева и правил соответственно.

Параметры plot_decision_tree:

    decision_points – точки, для которых необходимо нарисовать дерево решений
    max_depth – максимальная глубина дерева
    scale – масштаб графика
    savefig – нужно ли сохранить изображение

dm.plot_decision_tree(decision_points=['Stage_0'], max_depth=None, scale=1, savefig=False)

Параметры print_decision_rule:

    decision_points – точки, для которых необходимо вывести решающие правила
    paths – пути, по которым необходимо вывести решающие правила

dm.print_decision_rule(decision_points=['Stage_0'], paths=['Stage_1'])

XIII. Хронометраж

Расчёт длительности процесса с предварительной очисткой выбросов при помощи алгоритмов машинного обучения.

    data_holder (SberPM DataHolder) - класс с хранящимися данными
    start_query (str) - запрос указывающий на начало нового процесса
    end_query (str) - запрос указывающий на окончание процесса
    query (str) - запрос указывающий на начало нового процесса или на завершение процесса в данной строке
    change_columns (List[str]) - список с названиями колонок по которым можно определить что начался новый процесс при изменении значения в колонке (например изменения идентификатора процесса или пользователя)
    sort_params (List[str]) - список названий колонок по которым будет производиться предварительная сортировка данных

Параметры query, start_query, end_query могут быть типа "sql" или "pandas", они оба должны ссылаться на фрейм данных как "df", они должны возвращать один столбец: булевую маску или столбец из 0 и 1.

В случае, если задаётся "sql" запрос, то он должен выглядеть как "SELECT ... from df".

Метод get_chrono() начнёт процесс расчёта длительности процесса и в результате выведет словарь(dict) с элементами:

    среднее временя процесса в секундах
    количество отобранных элементов
    количество уникальных процессов
    максимальное количество уникальных идентификаторов рассчитанных в хронометраже

from sberpm.ml.chronometrage import Chronometrage

df = pd.read_excel('chrono_data.xlsx', engine='openpyxl')

dh = DataHolder(df, 'process_id', 'event_type', 'data_timestamp')

example_start_query = """(df['event_type'] == 'Процесс_16961') & (df['event_action'].isin(['Начало']))"""

example_end_query = """(df['event_type'] == 'Процесс_16961') & (df['event_action'].isin(['Конец']))"""

cr = Chronometrage(dh,

                   sort_params=['process_id', 'user_id', 'data_timestamp'],

                   start_query=example_start_query,

                   end_query=example_end_query,

                   change_columns=['process_id', 'user_id'])

res = cr.get_chrono()

res