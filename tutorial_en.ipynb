{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Mining with sberpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`sberpm`__ library is designed to analyse and investigate processes, display them as graphs and solve related classification and clustering problems using machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the full version of the library, write an email with a request to aabugaenko@sberbank.ru\n",
    "\n",
    "The page of the SberPM platform:\n",
    "https://developers.sber.ru/portal/products/sber-process-mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![title](poster_PM/poster.png)](https://developers.sber.ru/portal/products/sber-process-mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[----------  Data preprocessing  ----------](#----------Data-preprocessing----------)\n",
    "\n",
    "I. [Column matching](#I.-Column-matching) \n",
    "\n",
    "II. [Log generator](#II.-Log-generator) # only full library version\n",
    "\n",
    "III. [Synthetic process IDs](#III.-Synthetic-process-IDs) # only full library version\n",
    "\n",
    "IV. [DataHolder](#IV.-DataHolder)\n",
    "\n",
    "[----------  Automatic AI research  ----------](#----------Automatic-AI-research----------) # only full library version\n",
    "\n",
    "[----------  Traditional Process Mining  ----------](#----------Traditional-Process-Mining----------)\n",
    "\n",
    "I. [Miners and graph visualisation](#I.-Miners-and-graph-visualisation)\n",
    " 1. [SimpleMiner](#1.-SimpleMiner)\n",
    " 2. [CausalMiner](#2.-CausalMiner)\n",
    " 3. [HeuMiner](#3.-HeuMiner)\n",
    " 4. [AlphaMiner](#4.-AlphaMiner)\n",
    " 5. [AlphaPlusMiner](#5.-AlphaPlusMiner)\n",
    " 6. [InductiveMiner](#6.-InductiveMiner)\n",
    " 7. [ML-miners](#7.-ML-miners)\n",
    " 8. [NLP-Miner](#8.-NLP-miner) # only full library version\n",
    " 9. [Correlation-Miner](#9.-Correlation-miner)\n",
    " 10. [II-miner](#10.-II-miner)\n",
    "\n",
    "II. [Metrics](#II.-Metrics)\n",
    "- [Metrics + graphs](#Metrics-+-graphs)\n",
    "\n",
    "III. [Conformance Checking](#III.-Conformance-Checking)\n",
    "\n",
    "IV. [BPMN](#IV.-BPMN)\n",
    "\n",
    "V. [Visualisation](#V.-Visualisation)\n",
    "\n",
    "[---------- Machine Learning ----------](#-----------Machine-Learning-----------)\n",
    "\n",
    "I. [Clustering of stages](#I.-Clustering-of-stages) # only full library version\n",
    "\n",
    "II. [Automatic search for inefficiencies](#II.-Automatic-search-for-inefficiencies) # only full library version\n",
    "\n",
    "III. [Anomaly Detection](#III.-Anomaly-Detection) \n",
    "\n",
    "IV. [Factor analysis](#IV.-Factor-analysis) # only full library version\n",
    "\n",
    "V. [Auto ML](#V.-Auto-ML) # only full library version\n",
    "\n",
    "VI. [Recommender system](#V.-Recommender-system) # only full library version\n",
    "\n",
    "VII. [Text Analysis](#VI.-Text-Analysis) # only full library version\n",
    "\n",
    "VIII. [Sentimental Analysis](#VII.-Sentimental-Analysis) # only full library version\n",
    "\n",
    "IX. [Searching for a Happy Path](#VIII.-Searching-for-a-Happy-Path) # only full library version\n",
    "\n",
    "X. [Predicting graph structure](#IX.-Predicting-graph-structure) # only full library version\n",
    "\n",
    "XI. [Simulation modeling and what-if analysis](#X.-Simulation-modeling-and-what-if-analysis)\n",
    "\n",
    "XII. [Decision Mining](#XI.-Decision-Mining)\n",
    "\n",
    "XIII. [Timing](#XII.-Timing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------  Data preprocessing  ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Column matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column **Matching Module (sberpm.column_matching)** is a system for analyzing similarity of columns in two tables. Such analysis is performed using both text and numerical columns – in case columns in different tables have similar values, they will be matched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters of ColumnMatching**:\n",
    "\n",
    "* **data_left (pandas DataFrame)** – one table to compare\n",
    "* **data_right (pandas DataFrame)** - another table to compare\n",
    "* **p_value_threshold (float = default 0.05)** – Kolmogorov-Smirnov p-value threshold for numerical columns\n",
    "* **iou_threshold (float = default 0.5)** – threshold for IOU (Intersection over Union) metric for text columns\n",
    "\n",
    "**Methods of ColumnMatching**:\n",
    "* **get_result_pairs** returns pairs of similar columns in tables.\n",
    "* **get_result_table** returns a pivot table where the axis are the column names from compared tables and the values are their similarity measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sberpm.column_matching import ColumnMatching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_left = DataFrame({\n",
    "    \"names left\": [\"Alexander FFFFF\", \"Andrew Alexander\", \"Ilya Emilia\", \"Ilya Alexander\"] * 50,\n",
    "    \"last names left\": [\"Arkhipov\", \"Kuznetsova\", \"Kuznetsova\", \"Bugaenko\"] * 50,\n",
    "})\n",
    "data_right = DataFrame({\n",
    "    \"names right\": [\"Seva Alexander\", \"Alexander Vera\", \"Ilya Andrew\"] * 50,\n",
    "    \"last names right\": [\"Zarubin\", \"Zarubin\", \"Kuznetsova\"] * 50,\n",
    "})\n",
    "\n",
    "column_map = ColumnMatching(\n",
    "    data_left,\n",
    "    data_right,\n",
    "    p_value_threshold=0.05,\n",
    "    iou_threshold=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map.map_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map.get_result_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map.get_result_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Log generator\n",
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "\n",
    "__EmploeeGenerator__ generator that creates EmploeeCard.\n",
    "\n",
    "__LogGenerator__ allows to automatically generate process logs, it can also be configured for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Synthetic process IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "__Pro_n_check__ Numerates the process instances according to the given conditions. Creates a 'pro_n' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. DataHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataHolder` is a base class for storing data. Almost all library algorithms work with it (take it as input).\n",
    "\n",
    "To create a `DataHolder` class, you must first specify file path or pass DataFrame to the constructor and then specify __id_column__ and __activity_column__. However, for most of the Process Mining algorithms in the library, these columns are not sufficient - at least one time column (__start_timestamp_column__ and/or __end_timestamp_column__) and a user column (__user_column__) are required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataHolder parameters\n",
    "- **data (str or pd.DataFrame)** – data file path (.csv, .xls(x), .txt) or pd.DataFrame\n",
    "- **id_column (str)** – id column\n",
    "- **activity_column (str)** – activity column\n",
    "- __<font color='red'>*</font>start_timestamp_column (str)__ – start time of activities\n",
    "- __<font color='red'>*</font>end_timestamp_column (str)__ – end time of activities\n",
    "- __user_column (str)__ – column with user names/id\n",
    "- __text_column (str)__ – column with text data\n",
    "- __duration_column (str)__ – column with activity durations (if not specified, it is calculated as activity_time_2 - activity_time_1, and if there is only one column with time, NaN is set for the last activity in the chain)\n",
    "- __duration_unit (str)__ – dimension (unit of measure) of the values in the duration_column, if specified\n",
    "\n",
    "- __sep (str, default=',')__ – delimiter character (only used when reading data from a file)\n",
    "- __encoding (str)__ – encoding (only used when reading data from a file)\n",
    "- __nrows (int)__ – number of lines to read (only used when reading data from a file)\n",
    "\n",
    "- __preprocess (bool, default=True)__ – data preprocessing (sorting, removal of non-values, type conversions)\n",
    "- __time_format (str)__ – time column format (must be set for correct date recognition and to speed up operation). Formats can be found here: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\n",
    "- __time_errors: (str, default='raise')__ – action on conversion error\n",
    "- __dayfirst: (bool, default=None)__ – True, if starts from the day\n",
    "- __yearfirst: (bool, default=None)__ – True, if starts from the year\n",
    "- __n_jobs (int, default=1)__ – the maximum number of threads available\n",
    "\n",
    "\n",
    "__<font color='red'>*</font>__ For most algorithms at least one of the timestamp columns must be specified. If there is no information about the type of column (start or end time), it should be specified as __start_timestamp_column__. For correct format recognition it is also necessary to specify __time_format__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data under study should be like an event log (log file) that stores information about the sequence (chain) of events (activities) in the business processes. An example of an event log: $W = \\{(a,b,c,d), (a,c,b,d), (a,e,d)\\}$ where events $a$, $b$, $c$, $d$ and $e$ are sorted by time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataHolder creation \n",
    "### – with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm import DataHolder\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'id_column': [1, 1, 2, 2, 3, 3],\n",
    "                   'activity_column': ['st1', 'st2', 'st1', 'st3', 'st1','st2'],\n",
    "                   'start_timestamp_column': ['10.05.2020', '10.09.2020', '10.03.2020', '10.04.2020', '10.05.2020', '10.05.2020']})\n",
    "\n",
    "data_holder = DataHolder(data=df, \n",
    "                         id_column='id_column', \n",
    "                         activity_column='activity_column', \n",
    "                         start_timestamp_column='start_timestamp_column', \n",
    "                         time_format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### – with data file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'example.xlsx'\n",
    "data_holder = DataHolder(data=path, \n",
    "                         id_column='id', \n",
    "                         activity_column='stages', \n",
    "                         start_timestamp_column='dt', \n",
    "                         user_column='users', \n",
    "                         text_column=\"some_text\",\n",
    "                         time_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If dataset has a separator of some kind, such as '|' as in csv format, then after setting the columns, need to set the parameter __sep='|'__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataHolder attributes\n",
    "In the `DataHolder` the column names are stored in the corresponding variables (i.e. there is no need to remember the column names):\n",
    "- id_column\n",
    "- activity_column\n",
    "- start_timestamp_column\n",
    "- end_timestamp_column\n",
    "- user_column\n",
    "- text_column\n",
    "- duration_column\n",
    "\n",
    "In addition, the `DataHolder` stores raw and grouped data as a DataFrame, which can be accessed as follows\n",
    "- data\n",
    "- grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataHolder methods\n",
    "- __check_or_calc_duration__ – calculates the duration of each activity (in seconds) if necessary.\n",
    "- __get_grouped_data__ – outputs grouped data by id and specified columns (e.g, by activity_column and by start_timestamp_column)\n",
    "- __get_unique_activities__ – displays a list of unique activities\n",
    "- __get_columns__ – displays a list with column names \n",
    "- __get_text__ – displays the text column, if there is one\n",
    "- __get_timestamp_col__ – outputs a temporary column; if there are 2, outputs start_time_column\n",
    "- __is_interval__ – returns True if it is an \"interval log\" (which has both time columns: start and end of activity)\n",
    "- __top_traces_dh__ – returns the data_holder with data for the n most frequent chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_holder.check_or_calc_duration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_holder.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_holder.get_grouped_data(data_holder.activity_column, data_holder.start_timestamp_column).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_3 = data_holder.top_traces_dh(3)  # data for the top 3 chains only\n",
    "dfg = dh_3.get_grouped_data(dh_3.activity_column)\n",
    "dfg.value_counts(dh_3.activity_column)  # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once you have business process data with status chains and start times for each of them, you can load them into the `DataHolder` and build a graph that describes this business process as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------Traditional Process Mining----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Miners and graph visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several algorithms are implemented in the library to build and draw the process graph. All of them are stored in the __`sberpm.miners`__ module and have one method:\n",
    "- __apply__ - builds graph, which is saved in the graph field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SimpleMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SimpleMiner` draws all edges found in the log (without any filtering).\n",
    "\n",
    "In terms of Process Mining:\n",
    "> If in at least one chain of activities from the log, some activity $X$ is directly followed by an activity $Y$ (a chain of the form $...XY...$), then write $X>Y$ ($Y$ follows $X$, _follows_ relation).\n",
    "\n",
    "SimpleMiner draws edges between such pairs of activities $X$ and $Y$ if $X>Y$ is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.miners import SimpleMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SimpleMiner object. The DataHolder and algorithm parameters are pass to the constructor \n",
    "# (this miner has no parameters)\n",
    "simple_miner = SimpleMiner(data_holder)\n",
    "\n",
    "# Start the graph algorithm\n",
    "simple_miner.apply()\n",
    "\n",
    "# Saving a graph\n",
    "graph = simple_miner.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Graph visualization\n",
    "To visualise a graph, use `GraphvizPainter` from module __`sberpm.visual`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The class `GraphvizPainter` has methods:\n",
    "- __apply__ - accepts the graph from the miner and makes calculation for rendering it \n",
    "- __write_graph__ - saves the graph in the required format (pdf, svg, gv, png)\n",
    "- __show__ - displays the graph in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a GraphvizPainter object\n",
    "painter = GraphvizPainter()\n",
    "\n",
    "# Calculating the graph from the SimpleMiner results\n",
    "painter.apply(graph)\n",
    "\n",
    "# You can save the graph to your hard disk in png, svg, pdf or gv format\n",
    "painter.write_graph('SimpleMiner.png', format='png')\n",
    "\n",
    "# You can display the graph in notebook\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Graph` from module __`sberpm.visual`__ has methods:\n",
    "- __get_nodes__ - get all nodes\n",
    "- __get_edges__ - get all edges\n",
    "- __add_node_metric__ - add metric related to graph nodes\n",
    "- __add_edge_metric__ - add metric related to graph edges\n",
    "- __clear_node_metrics__ - delete all metrics from nodes\n",
    "- __clear_edge_metrics__ - remove all metrics from edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CausalMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CausalMiner` is based on filtering of edges.\n",
    "> Derived types of relations from $X>Y$:\n",
    "- direct relations ($X \\to Y$, _causal_ relation) are relations where $X>Y$ and not $Y>X$\n",
    "- parallel relations ($X\\parallel Y$, _parallell_ relation) are relations where $X>Y$ and $Y>X$\n",
    "- independent relations ($X\\#Y$, independent) are relations where neither $X>Y$ nor $Y>X$\n",
    "\n",
    "The CausalMiner draws edges between such pairs of activities $X$ and $Y$ if $X\\to Y$ is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm.miners import CausalMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miner\n",
    "causal_miner = CausalMiner(data_holder)\n",
    "causal_miner.apply()\n",
    "graph = causal_miner.graph\n",
    "\n",
    "# Displaying graph\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. HeuMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HeuMiner` – is a heuristic miner that removes the rarest connections depending on the threshold set. \n",
    "\n",
    "The **threshold** parameter takes values **from 0 to 1**. The higher it is, the fewer edges in the graph (the remaining edges are considered more important).\n",
    "\n",
    "Source: https://www.researchgate.net/publication/229124308_Process_Mining_with_the_Heuristics_Miner-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm.miners import HeuMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miner\n",
    "heu_miner = HeuMiner(data_holder, threshold=0.8)\n",
    "heu_miner.apply()\n",
    "graph = heu_miner.graph\n",
    "\n",
    "# Displaying graph\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. AlphaMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AlphaMiner` draws a graph in the form of Petri net, taking into account direct, parallel and independent relations between activities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm.miners import AlphaMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miner\n",
    "alpha_miner = AlphaMiner(data_holder)\n",
    "alpha_miner.apply()\n",
    "graph = alpha_miner.graph\n",
    "\n",
    "# Displaying graph\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5. AlphaPlusMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AlphaPlusMiner` – implementation of Alpha+ Miner, which also draws a graph in the form of Petri net with relations, but unlike AlphaMiner can work with one-loop chains of the form activity_1$\\to$activity_1 (self-loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm.miners import AlphaPlusMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miner\n",
    "alpha_miner_plus = AlphaPlusMiner(data_holder)\n",
    "alpha_miner_plus.apply()\n",
    "graph = alpha_miner_plus.graph\n",
    "\n",
    "# Displaying graph\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. InductiveMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`InductiveMiner` creates a process tree. The foxes of the tree are the actual process activities, the other nodes are the operators. There are 4 types of operators: \n",
    "- SEQUENTIAL (`->`), \n",
    "- EXCLUSIVE OR (`X`), \n",
    "- PARALLEL (`||`), \n",
    "- CYCLE (`*`).\n",
    "\n",
    "There is an additional 'operator' that says that it was not possible to find any of the 4 operators above:\n",
    "- MIXED MODEL ('`?)\n",
    "\n",
    "*Note*: some of the tree leaves may be *hidden activities*, displayed as black rectangles. They are not real activities and are only used to preserve the correct tree structure. \n",
    "\n",
    "For example, from a log of two process chains $W = \\{(a, b, c), (a, c)\\}$, you can get the following process tree:        \n",
    "`->(a, X(b, hidden_activity), c)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If during the next iteration the algorithm cannot find the graph slice (=select one of the 4 operators), the following behaviour can be added: if there exists activity A, when removing which operator can be found, the algorithm returns the following tree:            \n",
    "`||(X(activity_A, hidden_activity), graph_without_activity_A)` - |activity A is considered parallel to the rest of the graph.\n",
    "\n",
    "\n",
    "This behavior can be turned on or off with parameter **parallell_activity** in class `InductiveMiner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.miners import InductiveMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miner\n",
    "inductive_miner = InductiveMiner(data_holder)\n",
    "inductive_miner.apply()\n",
    "graph = inductive_miner.graph\n",
    "\n",
    "# Displaying graph\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. __ML-miners__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ML-miner__ based on Lasso regression. \n",
    "\n",
    "With this miner, number of repetitoions of each stage in each process is calculated. Lasso-regression is fitted, in which a column of one type of activity is considered as a goal (dependent variable), and the rest of the columns of other activities - as features (indipendent variables). On the edges (links) only those relationships are displayed that have the absolute value of the Lasso coefficient more than the value of the threshold value.\n",
    "``ML-miner`` draws the edges between such pairs of activity $X$ and $Y$, if $X$ affects $Y$.\n",
    "\n",
    "- __data_holder: DataHolder__ - An object containing a journal of events and the names of its necessary columns.\n",
    "- __threshold: float (По умолчанию 0.05)__ - The threshold of the absolute value of the regression coefficient. The values exceeding the threshold value are considered significant connections between activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.miners import MLMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miner\n",
    "ml_miner = MLMiner(data_holder, threshold=0.1)\n",
    "ml_miner.apply()\n",
    "graph = ml_miner.graph\n",
    "\n",
    "# Visualization\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Auto-miner__\n",
    "\n",
    "``Auto-miner`` based on Sequence Feature Selector.\n",
    "\n",
    "``Auto-miner`` draws edges between such pairs of activities $X$ and $Y$, if $X$ affects $Y$.\n",
    "\n",
    "- __data_holder: DataHolder__ An object containing a journal of events and the names of its necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.miners import AutoMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miner\n",
    "autominer = AutoMiner(data_holder)\n",
    "autominer.apply()\n",
    "graph = autominer.graph\n",
    "\n",
    "# Visualization\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. __NLP-miner__\n",
    "\n",
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "``NLP-miner`` analyze activity names (descriptions) of process with the help of ``navec``.\n",
    "\n",
    "``NLP-miner`` merge activities $X$ and $Y$ and remove edges between $X$ and $Y$, if description of $X$ similar to $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. __Correlation-miner__\n",
    "\n",
    "``Correlation-miner`` can create a graph based on a journal of events in which there is no ID instance of the process.\n",
    "\n",
    "The graph will have a 'Count' metric for nodes and edges.\n",
    "\n",
    "``Correlation-miner`` uses linear programming to restore the lost copies of the process based on the time of each stage.\n",
    "\n",
    "- __data_holder: DataHolder__ An object containing a journal of events (logs) and the names of its necessary columns.\n",
    "- __greedy: bool, default=True__ If True, a duration matrix is calculated with greedy approach. There is a chance that the solution will not be optimal, but this significantly reduces time and memory for large logs.\n",
    "- __sparse: bool, default=False__ It is used only when greedy=False. If True, the calculation of a duration matrix will be carried out using a sparse matrix. This can slightly increase the calculation time, but slightly reduce the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.miners import CorrelationMiner\n",
    "\n",
    "# Miner\n",
    "corr_miner = CorrelationMiner(data_holder)\n",
    "corr_miner.apply()\n",
    "graph = corr_miner.graph\n",
    "\n",
    "# Visualization\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. __II-miner__\n",
    "\n",
    "Miner for analyzing parallel actions.\n",
    "  Counts intersections on temporary labels of activities.\n",
    "  Parallel stages are stages with such copies of processes where \"begin\"-time and \"end\"-time are overlaped.\n",
    "  In addition, there id `parallel_metric` node, which shows the number of time intersections to\n",
    "this stage and other stages. From the blue nodes, the arrows indicate parallel stages.\n",
    "\n",
    "- __data_holder: DataHolder__ An object containing a journal of events and the names of its necessary columns.\n",
    "- __Graph__ obtained graph of a process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.miners import ParallelMiner\n",
    "\n",
    "# Miner\n",
    "parallel_miner = ParallelMiner(data_holder)\n",
    "parallel_miner.apply()\n",
    "graph = parallel_miner.graph\n",
    "\n",
    "# Visualization\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently 5 main types of metrics in the __`sberpm.metrics`__ module:\n",
    "1. `ActivityMetric` - metrics for activity (grouping by activity_column)\n",
    "2. `TransitionMetric` - metrics for transitions (grouping by unique transitions)\n",
    "3. `IdMetric` - metrics for id (grouping by id_column)\n",
    "4. `TraceMetric` - metrics for chains of activities (grouping by unique strings)\n",
    "5. `UserMetric` - metrics for users (grouping by user_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.metrics import ActivityMetric, TransitionMetric, IdMetric, TraceMetric, UserMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- __data_holder__ - object of type DataHolder for which metrics should be calculated\n",
    "- __time_unit__ - time unit, by default time metrics are calculated in hours\n",
    "- __round__ - number of digits after decimal point (only for metrics that can be floating-point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods common to all classes:\n",
    "- __apply__ - calculation of all characteristics\n",
    "- __calc_metrics(...)__ - calculate the specified metrics (corresponds to methods/column names in DataFrame from apply)\n",
    "- __calculate_time_metrics__ - calculation of time characteristics\n",
    "\n",
    "- __total_duration__ - total operating time\n",
    "- __min_duration__ - minimum operating time\n",
    "- __max_duration__ - maximum operating time\n",
    "- __mean_duration__ - mean operating time\n",
    "- __median_duration__ - median operating time\n",
    "- __std_duration__ - standard deviation of operating time\n",
    "- __var_duration__ - operating time variance\n",
    "\n",
    "Additional methods:\n",
    "- ActivityMetric\n",
    "    - __count__ - number of times the activity occurs in the log\n",
    "    - __unique_ids__ - unique id for each activity\n",
    "    - __unique_ids_num__ - number of unique id for each activity\n",
    "    - __aver_count_in_trace__ - average number of times an activity occurs in a chain\n",
    "    - __loop_percent__ - percentage of looping\n",
    "    - __throughput__ - frequency - number of activities performed per unit time\n",
    "    - __unique_users__ - unique users who performed the activity\n",
    "    - __unique_users_num__ - number of unique users working on the activity\n",
    "    - __success_rate(...)__ - share of id's having the given activity and succeded\n",
    "    - __failure_rate(...)__ - share of id's, having the given activity and failed (ended with unsuccessful activities)\n",
    "    \n",
    "    \n",
    "- IdMetric\n",
    "    - __trace__ - chain (list of activities)\n",
    "    - __trace_length__ - chain length (number of activities in the chain)\n",
    "    - __unique_activities__ - unique activities in a chain\n",
    "    - __unique_activities_num__ - number of unique activities in a chain\n",
    "    - __loop_percent__ - looping percentage\n",
    "    - __unique_users__ - unique users with this ID\n",
    "    - __unique_users_num__ - number of unique users working with this ID\n",
    "\n",
    "\n",
    "- TraceMetric\n",
    "    - __count__ - how many times this chain occurs in the log file\n",
    "    - __ids__ - unique id with the given chain\n",
    "    - __trace_length__ - the length of the chain\n",
    "    - __unique_activities__ - unique activities in the chain\n",
    "    - __unique_activities_num__ - number of unique activities in the chain\n",
    "    - __unique_users__ - unique users working on the chain of activities\n",
    "    - __unique_users_num__ - number of unique users working on the chain of activities\n",
    "\n",
    " \n",
    "- TransitionMetric\n",
    "    - __count__ - number of times the transition occurs in the log file\n",
    "    - __unique_ids__ - unique id for each transition\n",
    "    - __unique_ids_num__ - number of unique ids for each transition\n",
    "    - __aver_count_in_trace__ - average number of times the object occurs in the chain\n",
    "    - __loop_percent__ - percentage of looping\n",
    "    - __throughput__ - number of transitions per time unit\n",
    "    - __unique_users__ - unique users working on the object\n",
    "    - __unique_users_num__ - number of unique users working on the object\n",
    "    - __success_rate(...)__ - share of id's having given transition and succeded\n",
    "    - __failure_rate(...)__ - share of id's having given transition and failed.\n",
    "    \n",
    "    \n",
    "- UserMetric\n",
    "    - __count__ - how many times the given user occurs in the log\n",
    "    - __unique_activities__ - unique activities that the user was working with\n",
    "    - __unique_activities_num__ - number of unique activities the user was working with\n",
    "    - __unique_ids__ - unique id with the user\n",
    "    - __unique_ids_num__ - number of unique ids with the given user\n",
    "    - __throughput__ - number of times the object was executed per time unit\n",
    "    - __workload__ - share of logging activity performed by the given user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ActivityMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating ActivityMetric object\n",
    "activity_metric = ActivityMetric(data_holder, time_unit='d')\n",
    "\n",
    "# Calculation of all metrics\n",
    "activity_metric.apply().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. TransitionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating TransitionMetric object\n",
    "transition_metric = TransitionMetric(data_holder, time_unit='d')\n",
    "\n",
    "# Calculation of all metrics\n",
    "transition_metric.apply().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. IdMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating IdMetric object\n",
    "id_metric = IdMetric(data_holder, time_unit='d')\n",
    "\n",
    "# Calculation of all metrics\n",
    "id_metric.apply().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. TraceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating TraceMetric object\n",
    "trace_metric = TraceMetric(data_holder, time_unit='d')\n",
    "\n",
    "# Calculation of all metrics\n",
    "trace_metric.apply().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5. UserMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating UserMetric object\n",
    "user_metric = UserMetric(data_holder, time_unit='d')\n",
    "\n",
    "# Calculation of all metrics\n",
    "user_metric.apply().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Metrics + graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library implements the ability to represent some metrics on a graph. It can be done in `Graph` class using methods:\n",
    "- __add_node_metric__ - add metric associated with nodes of the graph\n",
    "- __add_edge_metric__ - add metric related to edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculation of metrics\n",
    "nodes_count_metric = activity_metric.count().to_dict()\n",
    "edges_count_metric = transition_metric.count().to_dict()\n",
    "mean_time_node_metric = activity_metric.mean_duration().fillna(0).to_dict()\n",
    "\n",
    "# Obtaining a graph from a miner\n",
    "graph = causal_miner.graph\n",
    "\n",
    "# Adding metrics to the graph\n",
    "graph.add_node_metric('count', nodes_count_metric)\n",
    "graph.add_edge_metric('count', edges_count_metric)\n",
    "graph.add_node_metric('mean_time', mean_time_node_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a GraphvizPainter object\n",
    "painter = GraphvizPainter()\n",
    "\n",
    "# Draw the graph and relate the colour of the nodes and edges to the required metrics\n",
    "painter.apply(graph, node_style_metric='count', edge_style_metric='count')\n",
    "# or painter.apply(graph, node_style_metric='mean_time', edge_style_metric='count')\n",
    "\n",
    "# Saving graph\n",
    "painter.write_graph(\"metric_graph.png\", format = 'png')\n",
    "\n",
    "# Displaying graph\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove metrics from a graph, use the following methods:\n",
    "- __clear_node_metrics__ - remove all metrics from nodes\n",
    "- __clear_edge_metrics__ - remove all metrics from edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.clear_node_metrics()\n",
    "graph.clear_edge_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Conformance Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TokenReplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TokenReplay` allows to calculate *fitness*, which shows how well the graph describes the business process (1 - good, 0 - bad). Fitness is calculated separately for each chain (id) when playing it over the Petri net using the following formula:\n",
    "$$ Fitness = \\frac{1}{2}\\Big(1-\\frac{missed}{consumed}\\Big) + \\frac{1}{2}\\Big(1-\\frac{remaining}{produced}\\Big) $$\n",
    "- produced tokens - created as a result of transition\n",
    "- consumed tokens - removed as a result of the transition\n",
    "- remaining tokens - remained at the end of the playback\n",
    "- missing tokens - did not exist, but they are necessary for playback, so they are inserted\n",
    "\n",
    "The library outputs the following metrics:\n",
    "- above 4 tokens' metrics and fitness of each chain\n",
    "- fitness averaged over all chains (__mean_fitness__)\n",
    "- fitness across the entire log - the sum of above 4 tokens' metrics across all chains in the log (__average_fitness__) is passed to the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm.conformance_checking import TokenReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_replay = TokenReplay(data_holder, alpha_miner.graph)\n",
    "token_replay.apply()\n",
    "token_replay.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean:', token_replay.mean_fitness)\n",
    "print('average:', token_replay.average_fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also available in the library is the more general ConformanceChecking class, which holds TokenReplay and a host of other metrics:\n",
    "- precision\n",
    "- generalization\n",
    "- simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.conformance_checking import ConformanceChecking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ConformanceChecking(data_holder, alpha_miner.graph)\n",
    "cc.get_conformance_checking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.get_fitness_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IV. BPMN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the graph in BPMN (Business Process Model and Notation) format, you can use `BpmnExporter` from module __`sberpm.bpmn`__. It has the following methods:\n",
    "- __apply_petri__ - build BPMN for Petri net\n",
    "- __get_string_representation__ - get BPMN graph notation\n",
    "- __write__ - write the graph in BPMN format\n",
    "\n",
    "At the moment only the graphs from Alpha Miner can be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm.bpmn import BpmnExporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bpmn_exporter = BpmnExporter()\n",
    "bpmn_exporter.apply(alpha_miner.graph)\n",
    "bpmn_exporter.get_string_representation()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpmn_exporter.write('exported.bpmn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There is a class `BpmnImporter` with the following methods to load BPMN-file:\n",
    "- __load_bpmn_from_xml__ - load the graph represented as BPMN\n",
    "- __get_pydotplus_graph__ - get the graph in the pydotplus format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sberpm.bpmn import BpmnImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bpmn_importer = BpmnImporter()\n",
    "bpmn_importer.load_bpmn_from_xml('exported.bpmn')\n",
    "pydot_graph = bpmn_importer.get_pydotplus_graph()\n",
    "pydot_graph.write('imported_bpmn.svg', prog='dot', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `ChartPainter` from module __`berpm.visual`__ is designed to create basic types of charts. The visualisation is based on the __`plotly`__ library, which makes all charts interactive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.visual import ChartPainter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- __data__ - data to be visualised (DataFrame, DataHolder or metrics class object)\n",
    "- __template__ - style of the charts, default is _plotly_\n",
    "- __palette__ - colour palette of graphs, default _sequential.Sunset_r_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `ChartPainter' method allows to draw a graph of a certain type:\n",
    "- __hist__ - Histogram\n",
    "- __bar__ - bar graph\n",
    "- __box__ - box plot\n",
    "- __scatter__ - scatter plot\n",
    "- __line__ - line chart\n",
    "- __pie__ - pie chart\n",
    "- __sunburst__ - sunburst diagram\n",
    "- __heatmap__ - 2D histogram\n",
    "- __timeline__ - Gantt chart\n",
    "- __pareto__ - Pareto diagram\n",
    "\n",
    "The main parameters of the methods (see documentation for details):\n",
    "- __x__, __y__ - names of columns to be drawn on X and Y axes correspondingly\n",
    "- __sort__ - names of column to sort values\n",
    "- __n__ - number of rows to be rendered\n",
    "- __color__ - name of the column to set colour for the chart items\n",
    "- __subplots__ - tuple of the form (rows, cols, ncols), where rows and cols are the names of the columns to draw multiple graphs by rows and columns respectively, and ncols is the number of columns\n",
    "- __text__ - name of the column with textual information (or its type) to be shown on the chart\n",
    "- __orientation__ - graphic orientation\n",
    "- __opacity__ - transparency of the chart elements\n",
    "- __edge__ - boundaries of the chart elements\n",
    "- __title__ - title of the chart\n",
    "\n",
    "Each method is easy to use, but has a sufficiently wide functionality that allows you to build charts for any task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painter = ChartPainter(id_metric)\n",
    "painter.hist(x='total_duration', edge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painter = ChartPainter(user_metric)\n",
    "painter.bar(x=data_holder.user_column, y='total_duration', text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painter = ChartPainter(id_metric)\n",
    "painter.scatter(x='mean_duration', y='median_duration', color='unique_users_num', size='trace_length', \n",
    "                edge=True, opacity=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painter = ChartPainter(user_metric)\n",
    "painter.pie(labels='count', n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of activity distribution over time ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painter = ChartPainter(data_holder)\n",
    "painter.hist_activity_of_dur(top= False, use_median=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For top activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painter.hist_activity_of_dur(top= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painter.hist_activity_of_dur(by_activity='Stage_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ---------- Machine Learning ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Clustering of stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "This module is used to cluster the process steps, to find close or identical process steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  II. Automatic search for inefficiencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "The automatic inefficiency search module __`sberpm.autoinsights`__ enables the automatic identification of __weaknesses and process vulnerabilities__ and demonstrates them visually in a process graph. The analysis by the `AutoInsights` class takes into account factors such as:\n",
    "1. The duration of the stage\n",
    "2. Increase of step duration\n",
    "3. Stage irregularity (rarity)\n",
    "4. Stage has a bottleneck with low variation\n",
    "5. Stage has a bottleneck with high variation\n",
    "6. Stage has a longer duration due to frequent incidents\n",
    "7. Stage has a longer duration because of recurring incidents\n",
    "8. Step increases the process time and/or other steps\n",
    "9. Step is run with errors, resulting in slowdown of the process\n",
    "10. Stage is run with critical system errors, which leads to the failure of the process\n",
    "11. Stage is run with structural errors which lead to failure of the process\n",
    "12. Storming at this stage leads to process slowdown failure\n",
    "13. Reversal at this stage leads to process failure\n",
    "14. Level of abnormality\n",
    "15. Sum of financial effects\n",
    "\n",
    "Results of the calculations are two parameters:\n",
    "- __Anomaly level - *[0, 1]*__.\n",
    "\n",
    "    For each object (activity and edge), its anomaly level is considered - a metric with values from 0 to 1 inclusive. The higher the anomaly level, the more insights the object can provide.\n",
    "    \n",
    "    \n",
    "- __Sum of financial effects - *[0, +inf)*__\n",
    "    \n",
    "    The sum of financial effects is the sum of financial effects of each metric in the table obtained through *get_clustered_result*. The financial effect is calculated for each activity based on the cost per 'second of human work' *sec_cost*. Also, depending on the metric, the financial effect is influenced by the durations of steps, cycles and other problems of the activity or activities on which the current activity depends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## III. Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Anomaly detection__ (also __outlier detection__) is the recognition of rare data, events, or observations that are suspicious because they are significantly different from most of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for anomalies (outliers) in the data, the library has a module __`sberpm.ml.anomaly_detection`__, which has classes `OutlierCBLOF`, `OutlierForest`, `OutlierLOF`, `OutlierOCSVM`, `OutlierCustom`, `OutlierEnsemble`. Each class implements its own __Anomaly Detection Without Teacher__ algorithm, which detects anomalies in unlabeled datasets, assuming most of the dataset is normal, by looking for representatives that fit less closely to the rest of the dataset. \n",
    "\n",
    "`OutlierEnsemble' is a composition of anomaly detection algorithms whose final answer is a vote (an object is considered an outlier if most algorithms have defined it as an outlier) of the following algorithms: [KNN](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn), [ABOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod), [HBOS](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos), [Isolation Forest](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.ml.anomaly_detection import OutlierCBLOF, OutlierForest, OutlierLOF, \\\n",
    "                                        OutlierOCSVM, OutlierCustom, OutlierEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a parameter, the object takes as input DataHolder, for which it calculates basic statistics, such as average time, activity chain length, number of unique users (if any), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detector = OutlierForest(data_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each class has the following methods:\n",
    "- __add_feature__ - adding the trait by which you want to find anomalies \n",
    "- __add_groupby_feature__ - add a feature for finding anomalies, calculated by grouped data \n",
    "- __apply__ - start the algorithm\n",
    "- __get_outlier_ids__ - displaying the id of the anomalous processes\n",
    "- __print_result__ - display the statistics on the anomalies\n",
    "- __show_permutation_importance__ - illustration of permutation importance of features by which outliers differ (works everywhere except `OutlierEnsemble`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adding a feature named max_time, calculated by applying the max function to the column \n",
    "# data_holder.duration_column in id grouped data (maximum activity time in the process)\n",
    "outlier_detector.add_groupby_feature('max_time', data_holder.duration_column, max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module implements 5 anomaly detection techniques:\n",
    "1. __Isolation Forest (IF)__\n",
    "2. __One-Class Support Vector Machines (OCSVM)__\n",
    "3. __Local Outlier Factor (LOF)__\n",
    "4. __Cluster-Based Local Outlier Factor (CBLOF)__\n",
    "5. __OutlierEnsemble__ which has __KNN__, __HBOS__, __ABOD__, __Isolation Forest__\n",
    "\n",
    "You can also use any other anomaly search algorithm (e.g. from __pyod__ library)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "\n",
    "> The __isolation forest__ technique is based on the idea that abnormal observations are easier to separate from the rest (normal) objects of the dataset. The algorithm builds an ensemble of isolating binary decision trees, in each node of which a feature and a partitioning threshold are chosen randomly. The tree is constructed until only one object or objects with the same values remain in the list. Intuitively, __anomalous__ points are those that have a shorter path length in the tree, which is defined as the number of edges that the object passes from the root node to the leaf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detector = OutlierForest(data_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### One-Class Support Vector Machines\n",
    "\n",
    "> The main idea of the classical __vector reference method (SVM)__ is to separate objects belonging to different classes by a hyperplane so as to maximize the distance between them. The __OCSVM__ algorithm, as the name implies, learns from data belonging to one class, the class of normal objects. It defines the boundaries of these objects and classifies all other points lying on the other side of the dividing surface as __anomalous__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detector = OutlierOCSVM(data_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Local Outlier Factor\n",
    "\n",
    "> The __Local Outlier Level (LOF)__ is based on the concept of local density of an object, where locality is given by its $k$ nearest neighbors, whose distances are used as density estimates. By comparing the local density of an object with the local density of its neighbors, we can distinguish regions with similar density and points that have significantly lower density than its neighbors. These points are considered __outliers__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detector = OutlierLOF(data_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cluster-Based Local Outlier Factor\n",
    "\n",
    "> Unlike the standard LOF, which is based on a metric approach to identify local outliers, __CBLOF__ identifies the cluster structure of the data, divides clusters into \"large\" and \"small\" and then determines the locality of small clusters with respect to large ones. Clusters whose locality with respect to others is small are defined as __outliers__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detector = OutlierCBLOF(data_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In addition to these 4 algorithms of anomaly search, you can use any other algorithm that is not built into the library, but is available in pyod - for example, __histogram-based outlier detection (HBOS)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyod.models.hbos import HBOS\n",
    "\n",
    "hbos = HBOS(contamination=0.1)\n",
    "outlier_detector = OutlierCustom(data_holder, hbos, outlier_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After selecting the algorithm, it should be applied using the __apply__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detector.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a list of anomaly id (__get_outlier_ids__ method), a table with descriptive statistics on anomalous and normal objects (__print_result__ method), and a graphical illustration of the importance of the features used to find anomalies (__show_permutation_importance__ method). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector.get_outlier_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector.show_permutation_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `OutlierEnsemble`\n",
    "> `OutlierEnsemble` is a composition of anomaly detection algorithms, the final answer of which is a vote (an object is considered an outlier if most algorithms have defined it as an outlier) of the following algorithms: [KNN](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn), [ABOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod), [HBOS](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos), [Isolation Forest](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest). Any subset of {\"HBOS\", \"ABOD\", \"KNN\", \"IForest\"} can be selected as algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector = OutlierEnsemble(data_holder, [\"HBOS\", \"ABOD\", \"KNN\", \"IForest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IV. Factor analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "\n",
    "__Factor analysis__ is a multivariate method used to examine relationships between values of variables. It is assumed that known variables depend on a smaller number of unknown variables and random error. With the help of factor analysis it is possible to identify hidden factor variables responsible for the presence of linear statistical correlations between the observed variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Auto ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "__AutoML__ is an automatic construction of models which allows to use machine learning methods to predict time rows and panel data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "\n",
    "This module is an advisory system that ranks the process steps in order of priority for reengineering. The system shows which activities to focus on first when optimizing the process. First, the best model is selected to describe the dependence of the target metric (metric) of each Activity on the attribute metrics (metric_f) of all other Activities. The metrics may be the number of recurrences (the number of activity occurrences, __\"appearance\"__), execution time (__\"time\"__), number of repetitions (__\"recycles\"__), custom user metric (\"__user_metric__\"). Then, based on the metric, problem coefficients are constructed, in descending order of which the Activity is ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "\n",
    "This module is used to cluster texts. \n",
    "For each cluster, the algorithm outputs a __cluster number__ (cluster name or closest message to the cluster centroid), and the 10 most common words in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Sentimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "\n",
    "This module is a system for analyzing the tone of verbal comments in the text box. The tonality analysis module has two modes: \"basic\" and \"advanced\". In the \"basic\" mode, the tone of the text is defined as \"positive\" or \"negative\", the numerical value of the tone is defined in the range from -1 to 1 (from negative to positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Searching for a Happy Path\n",
    "\n",
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "\n",
    "The task of finding a happy path can be solved by reinforcement learning (RL), which is inherently adapted to the search for optimal actions. RL works with two entities: the agent and the environment. During learning, the agent interacts with the environment, performs actions and receives feedback, which is called a reward. The problem is formulated within a Markovian decision-making process, which is based on:\n",
    "* a set of states in the world \n",
    "* a set of actions \n",
    "* a probabilistic distribution of the next state subject to the current state and the completed action \n",
    "* the reward in the transition between states when actions are performed. \n",
    "\n",
    "The fulfillment of the Markov property is that the next state is conditionally independent of past states and actions, given the current state and action. The process graph is treated as an environment, states are nodes of the graph (activities), actions are edges (choosing the next activity to transition), and the reward is the average negative transition time between past and present states. If there are key states, the transition to them is also rewarded. The goal is to choose an optimal policy - a mapping from state space to action space or, in other words, a guide to action in each state - that maximizes the expected discounted amount of rewards passing through the process graph.\n",
    "The optimal policy and, as a consequence, the path is found with AutoRL using the best discounted reward sum method from: value iteration, Q-learning, cross entropy, genetic algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Predicting graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Full library version</font>\n",
    "\n",
    "\n",
    "The __GSPredictor__ (graph structure predictor) class contains an algorithm for predicting graph structure, namely, two quantities are predicted: probabilities and average times of graph nodes and edges. These values are represented as time series from the available data, then ml- and time series-specific algorithms are used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## XI. Simulation modeling and what-if analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation module __`sberpm.imitation`__ allows to simulate the process in as-is form, make changes to the process and perform what-if simulations, and evaluate the quality of the simulation compared to the original log file using the `Simulation` and `SimilarityMetric` classes respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sberpm import DataHolder\n",
    "from sberpm.visual import GraphvizPainter\n",
    "from sberpm.miners import SimpleMiner, CausalMiner\n",
    "from sberpm.metrics import ActivityMetric, TransitionMetric\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_excel('example.xlsx')\n",
    "data.head()\n",
    "\n",
    "dh = DataHolder(\n",
    "    data=data,\n",
    "    id_column='id',\n",
    "    activity_column='stages',\n",
    "    start_timestamp_column='dt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Simulation__ class contains the simulation algorithm for the identical process, as well as the tools to change the structure of the process.\n",
    "\n",
    "Algorithm steps:\n",
    "\n",
    "I. __Data preparation__\n",
    "\n",
    "1. Cross Table generation with transitions in the table cells\n",
    "\n",
    "    - 'Start' and 'End' events are added to data, they are not displayed in logs, but they help in changing the structure of the process graph (for example, if you need to add an alternative input to the process, the following function is used: __add_edge__('start', 'node_name', prob = float))\n",
    "    - Adjacency matrix is created (with the number of transitions in cells), displayed as probabilities\n",
    "\n",
    "2. Extraction of temporary data from the original log, to preserve the initial distribution of activities duration.\n",
    "\n",
    "    - From data_holder extract activities duration (hereinafter they will be used to generate new stages with the identical duration distribution)\n",
    "\n",
    "II. __Generation__\n",
    "\n",
    "0. Table is converted into a dictionary __{'action': np.array ({'action': 'prob'})}__, and calculated an interval between process entering points (to calculate start_timestamp for each iteration)\n",
    "\n",
    "1. With a for loop generate the log of the process\n",
    "\n",
    "    - A chain of activity is built on each iteration recursively.\n",
    "    - Activities, the start time and ID of the process are written into the log\n",
    "\n",
    "2. Time generation (duration of stages):\n",
    "\n",
    "    - If there is activity in the original data_holder, then data is compressed / stretched to the occurancy number of this stage in the generated log\n",
    "    - If there is no activity and the creation is established by like_node, then the time is generated with the distribution, as in the indicated activity (time will be scaled depending on the specified mean_time)\n",
    "    - If there is no activity when creating and like_node is not indicated, then the average time value is taken (or the value of the average for all average process time, if no activity was added) and normal distribution is built.\n",
    "    \n",
    "III. __Changing the graph structure__\n",
    "\n",
    "Below are the main functions for working with the graph structure.\n",
    "\n",
    "1. Removing peaks and activities\n",
    "\n",
    "     - __delete_node__(node, save_con=True) - remove activity by name, while save_con flag means whether the compounds will be preserved.\n",
    "        Example:\n",
    "           A -> B -> C\n",
    "           delete_node('B', save_con=True)\n",
    "           A -> C\n",
    "    - __delete_edge__(node, node, side='right') - removes the edges (transitions) between passed activities. Taking into account the fact that connection can be in both directions, the side flag means whether the connection will be left to right, right to left or in both directions.\n",
    "        Example:\n",
    "           A <-> B\n",
    "           delete_edge ('A', 'B', side='right')\n",
    "           A <- B\n",
    "\n",
    "2. Adding edges and activities\n",
    "\n",
    "     - __add_node__(new_node, nodes=list, probabilites=list, mean_time=float, time_like='node', side='both') - Adding and binding the new activity with the list of activities, where `probabilities` - probability of transition, `mean_time` - average activity duration, `side` - in which direction the connection will be, `time_like` - copies the distribution of existing activity (will be scalled depending on mean_time)\n",
    "    - __add_edge__(node, node, prob, side) - adding a connection between activities, prob - probability of transition, side - which way will be the connection\n",
    "    - __swap_nodes__(node, node, save_probabilites=True) - changes places (along with the distribution of time), save_probabilites - the flag indicating whether the probabilities of the previous nodes when swap of activities takes place (if False, then a rearrangement and probability are recounted)\n",
    "\n",
    "3. Changing duration and probabilities\n",
    "    - __change_edge_prob__(node, node, prob=float) - changes the likelihood of transition between activities (prob = 0, similar to the removal of the transition)\n",
    "    - __scale_time_node__(node, scale=1) - changes the process time (distribution will remain the same - as in the original dataset), scale is a reduction factor (if < 1) or an increase (if > 1) the duration of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.imitation import Simulation, SimilarityMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of `Simulation` class:\n",
    "- __data_holder : DataHolder__ \n",
    "\n",
    "Methods of `Simulation` class:\n",
    "- __generate__ – start simulation of iterations (number) chain of activities (id)\n",
    "- ___mean_duration__ – average duration of activities or transitions\n",
    "- __scale_time_node__ – change of activity duration\n",
    "- __change_edge_probability__ - change of transition\n",
    "- __delete_node__, __delete_edge__, __delete_loop__, __delete_all_loops__ – remove node and edge (delete_loop - remove edge \"itself\"-directed)\n",
    "- __add_edge__, __add_node__, __add_loop__ - add node and edge (loop - add edge \"itself\"-directed)\n",
    "- __swap_nodes__ – swap nodes (keep probabilities or recalculate)\n",
    "- __get_result__ – get results of generation \n",
    "- __compute_metric__ - count metrics\n",
    "- __get_probabilities_tab__ - returns the adjacency matrix, allows to get the transitions' probability\n",
    "\n",
    "Class DataHolder as as input is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing\n",
    "sim = Simulation(data_holder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As-is modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The method __generate__ starts simulation. It has parameters: __iterations__ – number of activity chains for simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sim.generate(1000)\n",
    "sim_data = sim.get_result()\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dh = DataHolder(\n",
    "    data=sim_data,\n",
    "    id_column='id',\n",
    "    activity_column='stages',\n",
    "    start_timestamp_column='dt',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting log can be rendered using the miner and the built-in graph visualization tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What-if analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such methods as __delete_node__, __delete_edge__, __add_node__, __add_edge__, __add_loop__, __delete_loop__ и __swap_nodes__ allows to change process graph. In case simulation takes place, process will be implemented along the alternative paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nodes\n",
    "sim.delete_node(\"Stage_5\")\n",
    "# delete edges (side=True - only one direction)\n",
    "sim.delete_edge('Stage_7', 'Stage_5', side='right')\n",
    "# add node (action), nodes - list of connected nodes with a new one, probabilities - transitions' probabilities among nodes\n",
    "sim.add_node('Stages_10', \n",
    "             nodes=['Stage_8', 'Stage_5', 'Stage_10'], \n",
    "             probabilities=[0.15, 0.35], mean_time=3333, side='both')\n",
    "# add edge, prob - transition probability\n",
    "sim.add_edge('Stage_7', 'Stage_7', prob=0.15, side='right')\n",
    "\n",
    "# for loop cases (same as sim.add_edge('node1', 'node1', prob=0.5))\n",
    "sim.delete_loop('Stage_10')\n",
    "\n",
    "# swap nodes and duration (probabilities are same)\n",
    "sim.swap_nodes('Stage_2', 'Stage_0', save_probabilities=False)\n",
    "\n",
    "sim.generate(5000)\n",
    "\n",
    "sim.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control the change of transitions' probabilities will hellps such method as __get_probabilities_tab__, which returns table with transitions: rows here mean `from-action` while columns - `to-action`. Cell values are transtitions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.get_probabilities_tab()\n",
    "\n",
    "sim.change_edge_probability('Stage_10', 'Stage_10', 3)\n",
    "sim.change_edge_probability('Stage_9', 'end', 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such method as __scale_time_node__ changes process duration (Warning! distribution still same as id provided by origin dataset), `scale` - is the coefficient of decrease (if < 1) or increase (if > 1) of activity duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.scale_time_node('Stage_4', scale=1.5)\n",
    "sim.scale_time_node('Stage_3', scale=0.75)\n",
    "\n",
    "sim.generate(10000)\n",
    "sim_data = sim.get_result()\n",
    "sim_data.shape\n",
    "\n",
    "generated_dh = DataHolder(\n",
    "    data=sim.get_result(),\n",
    "    id_column='id',\n",
    "    activity_column='stages',\n",
    "    start_timestamp_column='dt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_miner = SimpleMiner(generated_dh)\n",
    "simple_miner.apply()\n",
    "graph = simple_miner.graph\n",
    "\n",
    "painter = GraphvizPainter()\n",
    "painter.apply(graph)\n",
    "painter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  XI. Decision Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module __`sberpm.decision_mining`__ is designed to perform __decision point analysis__, which is to determine the reasons why a process goes one way or another. The `DecisionMining` class identifies how certain properties (attributes) of a process affect the choice of a particular path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.decision_mining import DecisionMining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an input `DecisionMining` takes an object of type DataHolder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "dm = DecisionMining(data_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DecisionMining` has the following methods:\n",
    "- __print_decision_points__ - outputs decision points (activities followed by choices)\n",
    "- __apply__ - performs analysis of decision points, building decision tree by specified attributes\n",
    "- __get_clf_metrics__ - displays the classification metrics\n",
    "- __plot_confusion_matrix__ - draws error matrices\n",
    "- __plot_feature_importance__ - draws importance of attributes in the tree\n",
    "- __plot_feature_distribution__ - draws distribution of features by classes \n",
    "- __plot_decision_tree__ - draws the decision tree\n",
    "- __print_decision_rule__ - outputs decision rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision points - the points where the process has a branching can be viewed using the __print_decision_points__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.print_decision_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method __apply__ runs the decision mining algorithm. It has the following parameters:\n",
    "- __categorical_attrs__ - names of categorical features\n",
    "- __noncategorical_attrs__ - names of noncategorical features\n",
    "- __decision_points__ - points to build decision trees on, by default all are considered\n",
    "- __sampling__ - whether sampling (over- or under-) is needed, should be used in case of unbalanced classes\n",
    "- __tree_params__ - parameters of the decision tree\n",
    "- __grid_search__ - whether selection of optimal hyperparameters of the decision tree is needed\n",
    "- __param_grid__ - parameter grid, only used if grid_search=True\n",
    "- __random_state__ - used in the decision tree and sampling\n",
    "- __n_jobs__ - used in sampling and grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.apply(categorical_attrs=[data_holder.user_column],\n",
    "         noncategorical_attrs=[data_holder.duration_column],\n",
    "         decision_points='all', \n",
    "         sampling='RandomOverSampler',\n",
    "         tree_params='default',\n",
    "         grid_search=False, \n",
    "         param_grid='default',\n",
    "         random_state=42,\n",
    "         n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of classification can be displayed using the __get_clf_metrics__, __plot_confusion_matrix__ and __plot_feature_importance__ methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.get_clf_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality is poor due to the peculiarities of the synthetic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All plot methods have parameters:\n",
    "- __decision_points__ - the points for which you want to plot the charateristics\n",
    "- __savefig__ - whether to save the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.plot_confusion_matrix(decision_points=['Stage_0'], savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.plot_feature_importance(decision_points=['Stage_0'], savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method __plot_feature_importance__ additionally has two more parameters:\n",
    "- __drop_outliers__ - whether to remove outliers for quantitative traits\n",
    "- __clf_results__ - whether to draw the distribution of features by classification results (True) or by source log (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.plot_feature_distribution(decision_points=['Stage_0'], drop_outliers=True, clf_results=True, savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such methods as __plot_decision_tree__ and __print_decision_rule__ output the results of the decision mining algorithm as a tree and rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of __plot_decision_tree__:\n",
    "- __decision_points__ - points for which the decision tree should be drawn\n",
    "- __max_depth__ - maximum depth of the tree\n",
    "- __scale__ - scale of the chart\n",
    "- __savefig__ - whether the image should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.plot_decision_tree(decision_points=['Stage_0'], max_depth=None, scale=1, savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of __print_decision_rule__:\n",
    "- __decision_points__ - points for which you want to print decisive rules\n",
    "- __paths__ - paths along which you want to print decisive rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.print_decision_rule(decision_points=['Stage_0'], paths=['Stage_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XII. Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of the process duration with pre-cleaning of the outputs using machine learning algorithms.\n",
    "\n",
    "- __data_holder (SberPM DataHolder)__ - class with the stored data \n",
    "- __start_query__ (str) - request specifies the beginning of the new process\n",
    "- __end_query__ (str) - request that specifies the end of the process\n",
    "- __query__ (str) - request that points to the beginning of a new process or to the end of the process in this line\n",
    "- __change_columns__ (List[str]) - list with the names of the columns that can be used to determine that a new process started by changing a value in a column (for example changing process ID or user) \n",
    "- __sort_params__ (List[str]) - list of columns names by which the preliminary data sorting will be done\n",
    "\n",
    "Parameters __query__, __start_query__, __end_query__ can be __\"sql\"__ or __\"pandas\"__ type, they both should refer to the data frame as __\"df\"__, they should return one column: boolean mask or column of 0 and 1.\n",
    "\n",
    "If a __\"sql\"__ query is specified, it should look like __\"SELECT ... from df\"__.\n",
    "\n",
    "Method __get_chrono()__ will start calculation of process duration and as a result it will output the dictionary(dict) with elements:\n",
    "- average time of the process in seconds\n",
    "- number of selected elements\n",
    "- number of unique processes\n",
    "- maximum number of unique identifiers calculated in the timeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sberpm.ml.chronometrage import Chronometrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('chrono_data.xlsx', engine='openpyxl')\n",
    "dh = DataHolder(df, 'process_id', 'event_type', 'data_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_start_query = \"\"\"(df['event_type'] == 'Процесс_16961') & (df['event_action'].isin(['Начало']))\"\"\"\n",
    "example_end_query = \"\"\"(df['event_type'] == 'Процесс_16961') & (df['event_action'].isin(['Конец']))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Chronometrage(dh, \n",
    "                   sort_params=['process_id', 'user_id', 'data_timestamp'], \n",
    "                   start_query=example_start_query,\n",
    "                   end_query=example_end_query,\n",
    "                   change_columns=['process_id', 'user_id'])\n",
    "res = cr.get_chrono()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
